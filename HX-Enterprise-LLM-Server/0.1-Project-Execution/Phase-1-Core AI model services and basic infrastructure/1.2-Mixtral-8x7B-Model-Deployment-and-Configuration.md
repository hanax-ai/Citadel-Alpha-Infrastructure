# HXP-Enterprise LLM Server - Task 1.2: Mixtral-8x7B Model Deployment and Configuration

**Task Number:** 1.2  
**Task Title:** Mixtral-8x7B Model Deployment and Configuration  
**Created:** 2025-01-18  
**Assigned To:** AI Model Team  
**Priority:** Critical  
**Estimated Duration:** 2 days  
**Phase:** Phase 1 - Core AI Model Services and Basic Infrastructure  
**Architecture Component:** AI Model Services - Primary Large Language Model  
**Modular Library Module:** hxp_enterprise_llm.services.ai_models.mixtral_8x7b  

---

## üéØ **TASK DESCRIPTION**

### **Primary Objective:**
Deployment and configuration of the Mixtral-8x7B model as the primary large language model service on port 11400. This task implements the most resource-intensive AI model with 90GB memory allocation and optimized vLLM configuration for mixture-of-experts architecture processing.

### **Architecture Alignment:**
- **Component:** AI Model Services - Primary Large Language Model (Section 4.1 of Architecture Document)
- **Integration Points:** vLLM inference engine, API gateway, monitoring system, resource management
- **Performance Targets:** 2000ms average latency, 32K context length, 8K batched tokens
- **Resource Allocation:** 90GB memory, 8 CPU cores, dedicated storage for model files

### **Modular Library Integration:**
- **Primary Module:** hxp_enterprise_llm.services.ai_models.mixtral_8x7b
- **Supporting Modules:** hxp_enterprise_llm.services.core.vllm_engine, hxp_enterprise_llm.monitoring.model_metrics
- **Configuration Schema:** Mixtral8x7BConfig
- **Testing Suite:** tests/unit/test_mixtral_8x7b.py
- **Orchestration Logic:** hxp_enterprise_llm.orchestration.model_orchestrator

---

## ‚úÖ **SMART+ST VALIDATION**

| Principle | Status | Validation Notes | Architecture Alignment |
|-----------|--------|------------------|----------------------|
| **Specific** | ‚úÖ | Deploy Mixtral-8x7B model with vLLM optimization | Aligns with primary LLM architecture specifications |
| **Measurable** | ‚úÖ | 2000ms latency, 32K context, 90GB memory usage | Architecture metrics validate completion |
| **Achievable** | ‚úÖ | Realistic given 128GB RAM and vLLM capabilities | Resource allocations support achievement |
| **Relevant** | ‚úÖ | Critical primary AI model for enterprise workloads | Supports overall AI service architecture |
| **Small** | ‚úÖ | Focused on single model deployment and optimization | Appropriately scoped for primary LLM component |
| **Testable** | ‚úÖ | Performance benchmarks, health checks, API validation | Architecture validation criteria are testable |

---

## üîó **DEPENDENCIES AND PREREQUISITES**

### **Hard Dependencies (Must Complete 100%):**
- **Task Dependencies:** Task 1.1 (vLLM Service Framework and Systemd Configuration)
- **Architecture Dependencies:** vLLM 0.3.3, PyTorch, Transformers library
- **Infrastructure Dependencies:** 90GB available memory, 4TB storage space, 8 CPU cores
- **Modular Library Dependencies:** hxp_enterprise_llm.services.core.vllm_engine

### **Soft Dependencies (Should Complete):**
- **Recommended Tasks:** Task 0.3 (Storage Architecture), Task 0.4 (Network Configuration)
- **Performance Dependencies:** Base system performance baseline
- **Integration Dependencies:** Monitoring framework readiness

### **External Infrastructure Requirements:**
- **SQL Database Server (192.168.10.35):** Not required for this task
- **Vector Database Server (192.168.10.30):** Not required for this task
- **Metrics Server (192.168.10.37):** Not required for this task
- **Network Connectivity:** Internet access for model download, local network for service communication

---

## ‚öôÔ∏è **CONFIGURATION REQUIREMENTS**

### **Environment Variables (.env):**
```bash
# Mixtral-8x7B Model Configuration
MIXTRAL_8X7B_PORT=11400
MIXTRAL_8X7B_MEMORY_GB=90
MIXTRAL_8X7B_CPU_CORES=8
MIXTRAL_8X7B_MODEL_PATH=/opt/models/mixtral-8x7b-instruct-v0.1
MIXTRAL_8X7B_MODEL_NAME=mistralai/Mixtral-8x7B-Instruct-v0.1

# vLLM Configuration for Mixtral
MIXTRAL_8X7B_MAX_MODEL_LEN=32768
MIXTRAL_8X7B_TENSOR_PARALLEL_SIZE=1
MIXTRAL_8X7B_GPU_MEMORY_UTILIZATION=0.9
MIXTRAL_8X7B_MAX_NUM_BATCHED_TOKENS=8192
MIXTRAL_8X7B_MAX_NUM_SEQS=256

# Performance Configuration
MIXTRAL_8X7B_TARGET_LATENCY_MS=2000
MIXTRAL_8X7B_TARGET_THROUGHPUT_RPS=10
MIXTRAL_8X7B_SWAP_SPACE=0
MIXTRAL_8X7B_ENFORCE_EAGER=0

# Monitoring Configuration
MIXTRAL_8X7B_METRICS_PORT=11401
MIXTRAL_8X7B_HEALTH_CHECK_INTERVAL=30
```

### **Configuration Files:**
```yaml
# /opt/citadel/config/services/mixtral-8x7b.yaml
service:
  name: mixtral-8x7b
  port: 11400
  host: "0.0.0.0"
  workers: 1
  
model:
  path: /opt/models/mixtral-8x7b-instruct-v0.1
  name: mistralai/Mixtral-8x7B-Instruct-v0.1
  max_model_len: 32768
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
  max_num_batched_tokens: 8192
  max_num_seqs: 256
  
performance:
  target_latency_ms: 2000
  target_throughput_rps: 10
  memory_limit_gb: 90
  cpu_cores: 8
  swap_space: 0
  enforce_eager: 0
  
monitoring:
  health_check_interval: 30
  metrics_collection_interval: 15
  prometheus_port: 11401
  log_level: INFO
```

### **Modular Library Configuration:**
```python
# Configuration schema from modular library
from hxp_enterprise_llm.services.ai_models.mixtral_8x7b.config import Mixtral8x7BConfig
from hxp_enterprise_llm.schemas.configuration.model_schemas import ModelConfigSchema

config = Mixtral8x7BConfig(
    port=11400,
    memory_gb=90,
    cpu_cores=8,
    model_path="/opt/models/mixtral-8x7b-instruct-v0.1",
    model_name="mistralai/Mixtral-8x7B-Instruct-v0.1",
    max_model_len=32768,
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9,
    max_num_batched_tokens=8192,
    max_num_seqs=256,
    target_latency_ms=2000,
    target_throughput_rps=10,
    swap_space=0,
    enforce_eager=0
)
```

---

## üìù **DETAILED SUB-TASKS**

| Sub-Task | Description | Module/Component | Commands/Steps | Success Criteria | Duration |
|----------|-------------|------------------|----------------|------------------|----------|
| 1.2.1 | Download and verify Mixtral-8x7B model | Model Management | Download model files and verify integrity | Model files downloaded and verified | 4h |
| 1.2.2 | Configure vLLM for Mixtral architecture | vLLM Configuration | Optimize vLLM settings for mixture-of-experts | vLLM optimized for Mixtral performance | 2h |
| 1.2.3 | Create systemd service configuration | Service Management | Create mixtral-8x7b service with resource limits | Service configuration operational | 1h |
| 1.2.4 | Implement model service module | Modular Library | Implement hxp_enterprise_llm.services.ai_models.mixtral_8x7b | Model service module operational | 3h |
| 1.2.5 | Configure monitoring and metrics | Monitoring | Set up Prometheus metrics and health checks | Monitoring operational with model-specific metrics | 1h |
| 1.2.6 | Performance tuning and optimization | Performance | Optimize model loading and inference performance | Performance targets achieved | 3h |
| 1.2.7 | Integration testing and validation | Testing | Test API endpoints and performance benchmarks | All tests pass with performance targets met | 2h |

### **Implementation Commands:**
```bash
# Environment setup
source /opt/citadel/env/bin/activate
cd /opt/citadel/hxp-enterprise-llm

# Model download and setup
mkdir -p /opt/models/mixtral-8x7b-instruct-v0.1
cd /opt/models/mixtral-8x7b-instruct-v0.1
huggingface-cli download mistralai/Mixtral-8x7B-Instruct-v0.1 --local-dir .

# Verify model files
ls -la /opt/models/mixtral-8x7b-instruct-v0.1/
sha256sum -c model.sha256

# Service configuration
sudo cp /opt/citadel/hxp-enterprise-llm/config/services/mixtral-8x7b.yaml /opt/citadel/config/services/
sudo systemctl daemon-reload

# Start service
sudo systemctl enable citadel-llm@mixtral-8x7b.service
sudo systemctl start citadel-llm@mixtral-8x7b.service

# Validation commands
systemctl status citadel-llm@mixtral-8x7b.service
curl -X GET http://192.168.10.29:11400/health
curl -X GET http://192.168.10.29:11401/metrics
```

---

## üéØ **SUCCESS CRITERIA AND VALIDATION**

### **Primary Objectives:**
- [ ] **Architecture Compliance:** Mixtral-8x7B model operational with 90GB memory allocation
- [ ] **Performance Targets:** 2000ms average latency, 32K context length support
- [ ] **Integration Validation:** API endpoints operational, monitoring integrated
- [ ] **Monitoring Integration:** Model-specific metrics and health checks active
- [ ] **Modular Library Integration:** Mixtral service module operational

### **Architecture Validation Commands:**
```bash
# Service health validation
curl -X GET http://192.168.10.29:11400/health
# Expected: {"status": "healthy", "model": "mixtral-8x7b", "context_length": 32768, "timestamp": "..."}

# Performance validation
curl -X POST http://192.168.10.29:11400/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain quantum computing", "max_tokens": 100, "temperature": 0.7}'
# Expected: Response time < 2000ms

# Context length validation
curl -X POST http://192.168.10.29:11400/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "'$(printf 'A%.0s' {1..30000})'", "max_tokens": 50}'
# Expected: Successful processing of 30K+ context

# Metrics validation
curl -X GET http://192.168.10.29:11401/metrics
# Expected: Prometheus metrics format with Mixtral-specific metrics
```

### **Performance Benchmarks:**
```bash
# Latency benchmark
for i in {1..10}; do
  time curl -X POST http://192.168.10.29:11400/v1/completions \
    -H "Content-Type: application/json" \
    -d '{"prompt": "Performance test", "max_tokens": 50}' > /dev/null
done
# Expected: Average response time < 2000ms

# Throughput benchmark
ab -n 100 -c 5 -T application/json -p test_payload.json \
  http://192.168.10.29:11400/v1/completions
# Expected: Requests per second > 10

# Memory usage validation
systemctl show citadel-llm@mixtral-8x7b.service --property=MemoryCurrent
# Expected: Memory usage < 90GB
```

### **Integration Testing:**
```bash
# Model loading validation
python -c "
from hxp_enterprise_llm.services.ai_models.mixtral_8x7b.service import Mixtral8x7BService
service = Mixtral8x7BService(config)
print(service.get_model_info())
"
# Expected: Model loaded successfully with correct parameters

# API compatibility validation
curl -X POST http://192.168.10.29:11400/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello"}], "max_tokens": 50}'
# Expected: OpenAI-compatible response format
```

---

## üìä **MONITORING AND METRICS**

### **Model-Specific Metrics:**
- Model loading time (target: < 300 seconds)
- Inference latency (target: < 2000ms average)
- Context length utilization
- Token generation rate
- Memory usage and efficiency
- Error rate and failure patterns

### **Monitoring Integration:**
```bash
# Prometheus metrics endpoint
curl -X GET http://192.168.10.29:11401/metrics | grep mixtral
# Expected: Mixtral-specific metrics available

# Health check endpoint
curl -X GET http://192.168.10.29:11400/health
# Expected: Model healthy with performance metrics

# Model info endpoint
curl -X GET http://192.168.10.29:11400/v1/models
# Expected: Mixtral model information
```

---

## üîß **TROUBLESHOOTING AND MAINTENANCE**

### **Common Issues:**
1. **Model loading fails:** Check available memory and storage space
2. **High latency:** Optimize vLLM configuration and resource allocation
3. **Memory exhaustion:** Adjust memory limits and swap configuration
4. **Context length errors:** Verify max_model_len configuration

### **Maintenance Procedures:**
- Daily: Check model performance and resource usage
- Weekly: Review model logs and performance metrics
- Monthly: Update model files and dependencies
- Quarterly: Performance optimization and tuning

---

## üìö **DOCUMENTATION AND REFERENCES**

### **Related Documents:**
- HXP-Enterprise LLM Server Architecture Document (Section 4.1)
- Mixtral-8x7B Model Documentation
- vLLM Performance Optimization Guide

### **Configuration References:**
- Model files: /opt/models/mixtral-8x7b-instruct-v0.1/
- Service config: /opt/citadel/config/services/mixtral-8x7b.yaml
- Log files: /var/log/citadel-llm/mixtral-8x7b/
- Metrics endpoint: http://192.168.10.29:11401/metrics 