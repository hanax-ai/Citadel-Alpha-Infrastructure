# HXP-Enterprise LLM Server - Task 1.3: Hermes-2 Model Deployment and Configuration

**Task Number:** 1.3  
**Task Title:** Hermes-2 Model Deployment and Configuration  
**Created:** 2025-01-18  
**Assigned To:** AI Model Team  
**Priority:** High  
**Estimated Duration:** 1 day  
**Phase:** Phase 1 - Core AI Model Services and Basic Infrastructure  
**Architecture Component:** AI Model Services - Conversational AI Model  
**Modular Library Module:** hxp_enterprise_llm.services.ai_models.hermes_2  

---

## üéØ **TASK DESCRIPTION**

### **Primary Objective:**
Deployment and configuration of the Hermes-2 model optimized for conversational AI on port 11401. This task implements a specialized dialogue model with conversation memory and coherence optimization for interactive applications.

### **Architecture Alignment:**
- **Component:** AI Model Services - Conversational AI Model (Section 4.2 of Architecture Document)
- **Integration Points:** vLLM inference engine, conversation tracking, user session management
- **Performance Targets:** 1500ms average latency, 8K context length, conversation coherence
- **Resource Allocation:** 15GB memory, 4 CPU cores, conversation memory optimization

### **Modular Library Integration:**
- **Primary Module:** hxp_enterprise_llm.services.ai_models.hermes_2
- **Supporting Modules:** hxp_enterprise_llm.services.core.vllm_engine, hxp_enterprise_llm.services.conversation.session_manager
- **Configuration Schema:** Hermes2Config
- **Testing Suite:** tests/unit/test_hermes_2.py
- **Orchestration Logic:** hxp_enterprise_llm.orchestration.conversation_orchestrator

---

## ‚úÖ **SMART+ST VALIDATION**

| Principle | Status | Validation Notes | Architecture Alignment |
|-----------|--------|------------------|----------------------|
| **Specific** | ‚úÖ | Deploy Hermes-2 model with conversation optimization | Aligns with conversational AI architecture specifications |
| **Measurable** | ‚úÖ | 1500ms latency, 8K context, 15GB memory usage | Architecture metrics validate completion |
| **Achievable** | ‚úÖ | Realistic given available resources and vLLM capabilities | Resource allocations support achievement |
| **Relevant** | ‚úÖ | Critical conversational AI model for interactive applications | Supports overall AI service architecture |
| **Small** | ‚úÖ | Focused on single model deployment with conversation features | Appropriately scoped for conversational AI component |
| **Testable** | ‚úÖ | Performance benchmarks, conversation coherence tests | Architecture validation criteria are testable |

---

## üîó **DEPENDENCIES AND PREREQUISITES**

### **Hard Dependencies (Must Complete 100%):**
- **Task Dependencies:** Task 1.1 (vLLM Service Framework and Systemd Configuration)
- **Architecture Dependencies:** vLLM 0.3.3, PyTorch, Transformers library
- **Infrastructure Dependencies:** 15GB available memory, 2TB storage space, 4 CPU cores
- **Modular Library Dependencies:** hxp_enterprise_llm.services.core.vllm_engine

### **Soft Dependencies (Should Complete):**
- **Recommended Tasks:** Task 0.3 (Storage Architecture), Task 0.4 (Network Configuration)
- **Performance Dependencies:** Base system performance baseline
- **Integration Dependencies:** Monitoring framework readiness

### **External Infrastructure Requirements:**
- **SQL Database Server (192.168.10.35):** Not required for this task
- **Vector Database Server (192.168.10.30):** Not required for this task
- **Metrics Server (192.168.10.37):** Not required for this task
- **Network Connectivity:** Internet access for model download, local network for service communication

---

## ‚öôÔ∏è **CONFIGURATION REQUIREMENTS**

### **Environment Variables (.env):**
```bash
# Hermes-2 Model Configuration
HERMES_2_PORT=11401
HERMES_2_MEMORY_GB=15
HERMES_2_CPU_CORES=4
HERMES_2_MODEL_PATH=/opt/models/teknium-OpenHermes-2-Mistral-7B
HERMES_2_MODEL_NAME=teknium/OpenHermes-2-Mistral-7B

# vLLM Configuration for Hermes-2
HERMES_2_MAX_MODEL_LEN=8192
HERMES_2_TENSOR_PARALLEL_SIZE=1
HERMES_2_GPU_MEMORY_UTILIZATION=0.8
HERMES_2_MAX_NUM_BATCHED_TOKENS=4096
HERMES_2_MAX_NUM_SEQS=128

# Conversation Configuration
HERMES_2_CONVERSATION_MEMORY_SIZE=10
HERMES_2_CONTEXT_WINDOW_SIZE=8192
HERMES_2_COHERENCE_THRESHOLD=0.7

# Performance Configuration
HERMES_2_TARGET_LATENCY_MS=1500
HERMES_2_TARGET_THROUGHPUT_RPS=20
HERMES_2_SWAP_SPACE=0
HERMES_2_ENFORCE_EAGER=0

# Monitoring Configuration
HERMES_2_METRICS_PORT=11402
HERMES_2_HEALTH_CHECK_INTERVAL=30
```

### **Configuration Files:**
```yaml
# /opt/citadel/config/services/hermes-2.yaml
service:
  name: hermes-2
  port: 11401
  host: "0.0.0.0"
  workers: 1
  
model:
  path: /opt/models/teknium-OpenHermes-2-Mistral-7B
  name: teknium/OpenHermes-2-Mistral-7B
  max_model_len: 8192
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.8
  max_num_batched_tokens: 4096
  max_num_seqs: 128
  
conversation:
  memory_size: 10
  context_window_size: 8192
  coherence_threshold: 0.7
  session_timeout_minutes: 30
  
performance:
  target_latency_ms: 1500
  target_throughput_rps: 20
  memory_limit_gb: 15
  cpu_cores: 4
  swap_space: 0
  enforce_eager: 0
  
monitoring:
  health_check_interval: 30
  metrics_collection_interval: 15
  prometheus_port: 11402
  log_level: INFO
```

### **Modular Library Configuration:**
```python
# Configuration schema from modular library
from hxp_enterprise_llm.services.ai_models.hermes_2.config import Hermes2Config
from hxp_enterprise_llm.schemas.configuration.model_schemas import ModelConfigSchema

config = Hermes2Config(
    port=11401,
    memory_gb=15,
    cpu_cores=4,
    model_path="/opt/models/teknium-OpenHermes-2-Mistral-7B",
    model_name="teknium/OpenHermes-2-Mistral-7B",
    max_model_len=8192,
    tensor_parallel_size=1,
    gpu_memory_utilization=0.8,
    max_num_batched_tokens=4096,
    max_num_seqs=128,
    conversation_memory_size=10,
    context_window_size=8192,
    coherence_threshold=0.7,
    target_latency_ms=1500,
    target_throughput_rps=20,
    swap_space=0,
    enforce_eager=0
)
```

---

## üìù **DETAILED SUB-TASKS**

| Sub-Task | Description | Module/Component | Commands/Steps | Success Criteria | Duration |
|----------|-------------|------------------|----------------|------------------|----------|
| 1.3.1 | Download and verify Hermes-2 model | Model Management | Download model files and verify integrity | Model files downloaded and verified | 2h |
| 1.3.2 | Configure vLLM for conversation optimization | vLLM Configuration | Optimize vLLM settings for dialogue processing | vLLM optimized for conversation performance | 1h |
| 1.3.3 | Create systemd service configuration | Service Management | Create hermes-2 service with resource limits | Service configuration operational | 0.5h |
| 1.3.4 | Implement conversation session manager | Modular Library | Implement conversation memory and session tracking | Session management operational | 2h |
| 1.3.5 | Implement model service module | Modular Library | Implement hxp_enterprise_llm.services.ai_models.hermes_2 | Model service module operational | 2h |
| 1.3.6 | Configure monitoring and metrics | Monitoring | Set up Prometheus metrics and health checks | Monitoring operational with conversation metrics | 1h |
| 1.3.7 | Conversation coherence testing | Testing | Test conversation flow and coherence | Conversation coherence validated | 1.5h |

### **Implementation Commands:**
```bash
# Environment setup
source /opt/citadel/env/bin/activate
cd /opt/citadel/hxp-enterprise-llm

# Model download and setup
mkdir -p /opt/models/teknium-OpenHermes-2-Mistral-7B
cd /opt/models/teknium-OpenHermes-2-Mistral-7B
huggingface-cli download teknium/OpenHermes-2-Mistral-7B --local-dir .

# Verify model files
ls -la /opt/models/teknium-OpenHermes-2-Mistral-7B/
sha256sum -c model.sha256

# Service configuration
sudo cp /opt/citadel/hxp-enterprise-llm/config/services/hermes-2.yaml /opt/citadel/config/services/
sudo systemctl daemon-reload

# Start service
sudo systemctl enable citadel-llm@hermes-2.service
sudo systemctl start citadel-llm@hermes-2.service

# Validation commands
systemctl status citadel-llm@hermes-2.service
curl -X GET http://192.168.10.29:11401/health
curl -X GET http://192.168.10.29:11402/metrics
```

---

## üéØ **SUCCESS CRITERIA AND VALIDATION**

### **Primary Objectives:**
- [ ] **Architecture Compliance:** Hermes-2 model operational with 15GB memory allocation
- [ ] **Performance Targets:** 1500ms average latency, 8K context length support
- [ ] **Integration Validation:** Conversation session management operational
- [ ] **Monitoring Integration:** Conversation-specific metrics and health checks active
- [ ] **Modular Library Integration:** Hermes-2 service module operational

### **Architecture Validation Commands:**
```bash
# Service health validation
curl -X GET http://192.168.10.29:11401/health
# Expected: {"status": "healthy", "model": "hermes-2", "context_length": 8192, "conversation_memory": true, "timestamp": "..."}

# Performance validation
curl -X POST http://192.168.10.29:11401/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Hello, how are you?"}], "max_tokens": 100}'
# Expected: Response time < 1500ms

# Conversation coherence validation
curl -X POST http://192.168.10.29:11401/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-Session-ID: test-session-123" \
  -d '{"messages": [{"role": "user", "content": "What did I just ask you?"}], "max_tokens": 50}'
# Expected: Coherent response referencing previous conversation

# Session management validation
curl -X GET http://192.168.10.29:11401/sessions/test-session-123
# Expected: Session information with conversation history
```

### **Performance Benchmarks:**
```bash
# Latency benchmark
for i in {1..10}; do
  time curl -X POST http://192.168.10.29:11401/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "Performance test"}], "max_tokens": 50}' > /dev/null
done
# Expected: Average response time < 1500ms

# Throughput benchmark
ab -n 100 -c 10 -T application/json -p conversation_payload.json \
  http://192.168.10.29:11401/v1/chat/completions
# Expected: Requests per second > 20

# Memory usage validation
systemctl show citadel-llm@hermes-2.service --property=MemoryCurrent
# Expected: Memory usage < 15GB
```

### **Integration Testing:**
```bash
# Model loading validation
python -c "
from hxp_enterprise_llm.services.ai_models.hermes_2.service import Hermes2Service
service = Hermes2Service(config)
print(service.get_model_info())
"
# Expected: Model loaded successfully with conversation capabilities

# Conversation flow validation
python -c "
from hxp_enterprise_llm.services.conversation.session_manager import SessionManager
session_mgr = SessionManager()
session_id = session_mgr.create_session()
print(session_mgr.get_session(session_id))
"
# Expected: Session created successfully with conversation memory
```

---

## üìä **MONITORING AND METRICS**

### **Model-Specific Metrics:**
- Model loading time (target: < 120 seconds)
- Inference latency (target: < 1500ms average)
- Conversation coherence score
- Session management efficiency
- Memory usage and efficiency
- Error rate and failure patterns

### **Monitoring Integration:**
```bash
# Prometheus metrics endpoint
curl -X GET http://192.168.10.29:11402/metrics | grep hermes
# Expected: Hermes-2-specific metrics available

# Health check endpoint
curl -X GET http://192.168.10.29:11401/health
# Expected: Model healthy with conversation metrics

# Session metrics endpoint
curl -X GET http://192.168.10.29:11401/metrics/sessions
# Expected: Session management metrics
```

---

## üîß **TROUBLESHOOTING AND MAINTENANCE**

### **Common Issues:**
1. **Model loading fails:** Check available memory and storage space
2. **High latency:** Optimize vLLM configuration and resource allocation
3. **Conversation incoherence:** Verify session management configuration
4. **Memory leaks:** Check conversation memory cleanup procedures

### **Maintenance Procedures:**
- Daily: Check model performance and conversation quality
- Weekly: Review conversation logs and session metrics
- Monthly: Update model files and conversation patterns
- Quarterly: Performance optimization and conversation tuning

---

## üìö **DOCUMENTATION AND REFERENCES**

### **Related Documents:**
- HXP-Enterprise LLM Server Architecture Document (Section 4.2)
- Hermes-2 Model Documentation
- Conversation Management Best Practices

### **Configuration References:**
- Model files: /opt/models/teknium-OpenHermes-2-Mistral-7B/
- Service config: /opt/citadel/config/services/hermes-2.yaml
- Log files: /var/log/citadel-llm/hermes-2/
- Metrics endpoint: http://192.168.10.29:11402/metrics 