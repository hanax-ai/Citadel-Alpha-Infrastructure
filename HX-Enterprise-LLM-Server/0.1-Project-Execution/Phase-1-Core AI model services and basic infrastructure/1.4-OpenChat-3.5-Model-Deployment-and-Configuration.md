# HXP-Enterprise LLM Server - Task 1.4: OpenChat-3.5 Model Deployment and Configuration

**Task Number:** 1.4  
**Task Title:** OpenChat-3.5 Model Deployment and Configuration  
**Created:** 2025-01-18  
**Assigned To:** AI Model Team  
**Priority:** High  
**Estimated Duration:** 1 day  
**Phase:** Phase 1 - Core AI Model Services and Basic Infrastructure  
**Architecture Component:** AI Model Services - Real-Time Interactive Model  
**Modular Library Module:** hxp_enterprise_llm.services.ai_models.openchat_3_5  

---

## üéØ **TASK DESCRIPTION**

### **Primary Objective:**
Deployment and configuration of the OpenChat-3.5 model optimized for real-time interactive processing on port 11402. This task implements a lightweight, fast-response model designed for high-throughput interactive applications with minimal latency requirements.

### **Architecture Alignment:**
- **Component:** AI Model Services - Real-Time Interactive Model (Section 4.3 of Architecture Document)
- **Integration Points:** vLLM inference engine, streaming response, concurrent user support
- **Performance Targets:** 1000ms average latency, 4K context length, high throughput
- **Resource Allocation:** 8GB memory, 4 CPU cores, streaming optimization

### **Modular Library Integration:**
- **Primary Module:** hxp_enterprise_llm.services.ai_models.openchat_3_5
- **Supporting Modules:** hxp_enterprise_llm.services.core.vllm_engine, hxp_enterprise_llm.services.streaming.response_streamer
- **Configuration Schema:** OpenChat35Config
- **Testing Suite:** tests/unit/test_openchat_3_5.py
- **Orchestration Logic:** hxp_enterprise_llm.orchestration.interactive_orchestrator

---

## ‚úÖ **SMART+ST VALIDATION**

| Principle | Status | Validation Notes | Architecture Alignment |
|-----------|--------|------------------|----------------------|
| **Specific** | ‚úÖ | Deploy OpenChat-3.5 model with real-time optimization | Aligns with real-time interactive architecture specifications |
| **Measurable** | ‚úÖ | 1000ms latency, 4K context, 8GB memory usage | Architecture metrics validate completion |
| **Achievable** | ‚úÖ | Realistic given available resources and vLLM capabilities | Resource allocations support achievement |
| **Relevant** | ‚úÖ | Critical real-time model for interactive applications | Supports overall AI service architecture |
| **Small** | ‚úÖ | Focused on single model deployment with streaming features | Appropriately scoped for real-time interactive component |
| **Testable** | ‚úÖ | Performance benchmarks, streaming tests, concurrency validation | Architecture validation criteria are testable |

---

## üîó **DEPENDENCIES AND PREREQUISITES**

### **Hard Dependencies (Must Complete 100%):**
- **Task Dependencies:** Task 1.1 (vLLM Service Framework and Systemd Configuration)
- **Architecture Dependencies:** vLLM 0.3.3, PyTorch, Transformers library
- **Infrastructure Dependencies:** 8GB available memory, 1TB storage space, 4 CPU cores
- **Modular Library Dependencies:** hxp_enterprise_llm.services.core.vllm_engine

### **Soft Dependencies (Should Complete):**
- **Recommended Tasks:** Task 0.3 (Storage Architecture), Task 0.4 (Network Configuration)
- **Performance Dependencies:** Base system performance baseline
- **Integration Dependencies:** Monitoring framework readiness

### **External Infrastructure Requirements:**
- **SQL Database Server (192.168.10.35):** Not required for this task
- **Vector Database Server (192.168.10.30):** Not required for this task
- **Metrics Server (192.168.10.37):** Not required for this task
- **Network Connectivity:** Internet access for model download, local network for service communication

---

## ‚öôÔ∏è **CONFIGURATION REQUIREMENTS**

### **Environment Variables (.env):**
```bash
# OpenChat-3.5 Model Configuration
OPENCHAT_3_5_PORT=11402
OPENCHAT_3_5_MEMORY_GB=8
OPENCHAT_3_5_CPU_CORES=4
OPENCHAT_3_5_MODEL_PATH=/opt/models/openchat-3.5-0106
OPENCHAT_3_5_MODEL_NAME=openchat/openchat-3.5-0106

# vLLM Configuration for OpenChat-3.5
OPENCHAT_3_5_MAX_MODEL_LEN=4096
OPENCHAT_3_5_TENSOR_PARALLEL_SIZE=1
OPENCHAT_3_5_GPU_MEMORY_UTILIZATION=0.7
OPENCHAT_3_5_MAX_NUM_BATCHED_TOKENS=2048
OPENCHAT_3_5_MAX_NUM_SEQS=64

# Real-Time Configuration
OPENCHAT_3_5_STREAMING_ENABLED=true
OPENCHAT_3_5_CONCURRENT_USERS=50
OPENCHAT_3_5_RESPONSE_CHUNK_SIZE=10
OPENCHAT_3_5_LOW_LATENCY_MODE=true

# Performance Configuration
OPENCHAT_3_5_TARGET_LATENCY_MS=1000
OPENCHAT_3_5_TARGET_THROUGHPUT_RPS=50
OPENCHAT_3_5_SWAP_SPACE=0
OPENCHAT_3_5_ENFORCE_EAGER=0

# Monitoring Configuration
OPENCHAT_3_5_METRICS_PORT=11403
OPENCHAT_3_5_HEALTH_CHECK_INTERVAL=15
```

### **Configuration Files:**
```yaml
# /opt/citadel/config/services/openchat-3.5.yaml
service:
  name: openchat-3.5
  port: 11402
  host: "0.0.0.0"
  workers: 1
  
model:
  path: /opt/models/openchat-3.5-0106
  name: openchat/openchat-3.5-0106
  max_model_len: 4096
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.7
  max_num_batched_tokens: 2048
  max_num_seqs: 64
  
realtime:
  streaming_enabled: true
  concurrent_users: 50
  response_chunk_size: 10
  low_latency_mode: true
  connection_timeout_seconds: 30
  
performance:
  target_latency_ms: 1000
  target_throughput_rps: 50
  memory_limit_gb: 8
  cpu_cores: 4
  swap_space: 0
  enforce_eager: 0
  
monitoring:
  health_check_interval: 15
  metrics_collection_interval: 10
  prometheus_port: 11403
  log_level: INFO
```

### **Modular Library Configuration:**
```python
# Configuration schema from modular library
from hxp_enterprise_llm.services.ai_models.openchat_3_5.config import OpenChat35Config
from hxp_enterprise_llm.schemas.configuration.model_schemas import ModelConfigSchema

config = OpenChat35Config(
    port=11402,
    memory_gb=8,
    cpu_cores=4,
    model_path="/opt/models/openchat-3.5-0106",
    model_name="openchat/openchat-3.5-0106",
    max_model_len=4096,
    tensor_parallel_size=1,
    gpu_memory_utilization=0.7,
    max_num_batched_tokens=2048,
    max_num_seqs=64,
    streaming_enabled=True,
    concurrent_users=50,
    response_chunk_size=10,
    low_latency_mode=True,
    target_latency_ms=1000,
    target_throughput_rps=50,
    swap_space=0,
    enforce_eager=0
)
```

---

## üìù **DETAILED SUB-TASKS**

| Sub-Task | Description | Module/Component | Commands/Steps | Success Criteria | Duration |
|----------|-------------|------------------|----------------|------------------|----------|
| 1.4.1 | Download and verify OpenChat-3.5 model | Model Management | Download model files and verify integrity | Model files downloaded and verified | 1.5h |
| 1.4.2 | Configure vLLM for real-time processing | vLLM Configuration | Optimize vLLM settings for low latency | vLLM optimized for real-time performance | 1h |
| 1.4.3 | Create systemd service configuration | Service Management | Create openchat-3.5 service with resource limits | Service configuration operational | 0.5h |
| 1.4.4 | Implement streaming response module | Modular Library | Implement response streaming capabilities | Streaming operational with chunked responses | 2h |
| 1.4.5 | Implement model service module | Modular Library | Implement hxp_enterprise_llm.services.ai_models.openchat_3_5 | Model service module operational | 2h |
| 1.4.6 | Configure monitoring and metrics | Monitoring | Set up Prometheus metrics and health checks | Monitoring operational with real-time metrics | 1h |
| 1.4.7 | Concurrency and streaming testing | Testing | Test concurrent users and streaming performance | Concurrency and streaming validated | 1h |

### **Implementation Commands:**
```bash
# Environment setup
source /opt/citadel/env/bin/activate
cd /opt/citadel/hxp-enterprise-llm

# Model download and setup
mkdir -p /opt/models/openchat-3.5-0106
cd /opt/models/openchat-3.5-0106
huggingface-cli download openchat/openchat-3.5-0106 --local-dir .

# Verify model files
ls -la /opt/models/openchat-3.5-0106/
sha256sum -c model.sha256

# Service configuration
sudo cp /opt/citadel/hxp-enterprise-llm/config/services/openchat-3.5.yaml /opt/citadel/config/services/
sudo systemctl daemon-reload

# Start service
sudo systemctl enable citadel-llm@openchat-3.5.service
sudo systemctl start citadel-llm@openchat-3.5.service

# Validation commands
systemctl status citadel-llm@openchat-3.5.service
curl -X GET http://192.168.10.29:11402/health
curl -X GET http://192.168.10.29:11403/metrics
```

---

## üéØ **SUCCESS CRITERIA AND VALIDATION**

### **Primary Objectives:**
- [ ] **Architecture Compliance:** OpenChat-3.5 model operational with 8GB memory allocation
- [ ] **Performance Targets:** 1000ms average latency, 4K context length support
- [ ] **Integration Validation:** Streaming responses operational, concurrent user support
- [ ] **Monitoring Integration:** Real-time metrics and health checks active
- [ ] **Modular Library Integration:** OpenChat-3.5 service module operational

### **Architecture Validation Commands:**
```bash
# Service health validation
curl -X GET http://192.168.10.29:11402/health
# Expected: {"status": "healthy", "model": "openchat-3.5", "context_length": 4096, "streaming": true, "timestamp": "..."}

# Performance validation
curl -X POST http://192.168.10.29:11402/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Quick response test"}], "max_tokens": 50}'
# Expected: Response time < 1000ms

# Streaming validation
curl -X POST http://192.168.10.29:11402/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages": [{"role": "user", "content": "Streaming test"}], "max_tokens": 100, "stream": true}'
# Expected: Chunked streaming response

# Concurrency validation
for i in {1..10}; do
  curl -X POST http://192.168.10.29:11402/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "Concurrent test $i"}], "max_tokens": 20}' &
done
wait
# Expected: All requests processed successfully
```

### **Performance Benchmarks:**
```bash
# Latency benchmark
for i in {1..20}; do
  time curl -X POST http://192.168.10.29:11402/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "Performance test"}], "max_tokens": 30}' > /dev/null
done
# Expected: Average response time < 1000ms

# Throughput benchmark
ab -n 200 -c 20 -T application/json -p realtime_payload.json \
  http://192.168.10.29:11402/v1/chat/completions
# Expected: Requests per second > 50

# Memory usage validation
systemctl show citadel-llm@openchat-3.5.service --property=MemoryCurrent
# Expected: Memory usage < 8GB
```

### **Integration Testing:**
```bash
# Model loading validation
python -c "
from hxp_enterprise_llm.services.ai_models.openchat_3_5.service import OpenChat35Service
service = OpenChat35Service(config)
print(service.get_model_info())
"
# Expected: Model loaded successfully with streaming capabilities

# Streaming validation
python -c "
from hxp_enterprise_llm.services.streaming.response_streamer import ResponseStreamer
streamer = ResponseStreamer()
response = streamer.stream_response('Test streaming', config)
print('Streaming operational')
"
# Expected: Streaming response generated successfully
```

---

## üìä **MONITORING AND METRICS**

### **Model-Specific Metrics:**
- Model loading time (target: < 60 seconds)
- Inference latency (target: < 1000ms average)
- Streaming response time
- Concurrent user capacity
- Memory usage and efficiency
- Error rate and failure patterns

### **Monitoring Integration:**
```bash
# Prometheus metrics endpoint
curl -X GET http://192.168.10.29:11403/metrics | grep openchat
# Expected: OpenChat-3.5-specific metrics available

# Health check endpoint
curl -X GET http://192.168.10.29:11402/health
# Expected: Model healthy with real-time metrics

# Streaming metrics endpoint
curl -X GET http://192.168.10.29:11402/metrics/streaming
# Expected: Streaming performance metrics
```

---

## üîß **TROUBLESHOOTING AND MAINTENANCE**

### **Common Issues:**
1. **Model loading fails:** Check available memory and storage space
2. **High latency:** Optimize vLLM configuration and resource allocation
3. **Streaming failures:** Verify streaming configuration and network settings
4. **Concurrency issues:** Check resource limits and connection handling

### **Maintenance Procedures:**
- Daily: Check model performance and streaming quality
- Weekly: Review real-time logs and performance metrics
- Monthly: Update model files and streaming patterns
- Quarterly: Performance optimization and concurrency tuning

---

## üìö **DOCUMENTATION AND REFERENCES**

### **Related Documents:**
- HXP-Enterprise LLM Server Architecture Document (Section 4.3)
- OpenChat-3.5 Model Documentation
- Real-Time Streaming Best Practices

### **Configuration References:**
- Model files: /opt/models/openchat-3.5-0106/
- Service config: /opt/citadel/config/services/openchat-3.5.yaml
- Log files: /var/log/citadel-llm/openchat-3.5/
- Metrics endpoint: http://192.168.10.29:11403/metrics 