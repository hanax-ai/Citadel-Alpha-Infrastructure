# Project 2: Vector Database Server - Part 1 Enhanced Implementation
# R&D Environment with Minimum Security, GraphQL/gRPC, and Automated Testing

**Document ID:** TL-P02-VDB-P1-ENHANCED  
**Version:** 1.0  
**Date:** 2025-07-15  
**Server/Component:** hx-vector-database-server (192.168.10.30)  
**Implementation Scope:** Part 1 - R&D Environment with Minimum Security  
**Related Documents:** 
- Project2_Vector_Database_Server_PRD_Updated.md
- Project2_Vector_Database_Server_Revised_Tasks.md

---

## 🎯 Document Overview

This standalone document provides comprehensive implementation guidance for Project 2: Vector Database Server Part 1, focusing on R&D environment deployment with minimum security requirements. The scope has been enhanced to include GraphQL/gRPC APIs, comprehensive scalability testing, external model integration validation, and automated testing frameworks.

### **Enhanced Scope Additions:**
- **GraphQL/gRPC Implementation** - Modern API interfaces for vector operations
- **Scalability Testing** - Comprehensive load testing with Locust framework
- **External Model Integration Tests** - Validation of all 9 external AI model integrations
- **Automated Testing Suite** - pytest for API testing, Locust for performance testing
- **PRD Traceability** - Explicit FR/NFR code references for requirement tracking

### **Implementation Philosophy:**
- **R&D First** - Prioritize functionality and development velocity
- **Minimum Security** - Essential security only, aligned with Project 1 standards
- **Comprehensive Testing** - Automated validation at every level
- **Modern APIs** - GraphQL and gRPC alongside REST for maximum flexibility

---

## 📊 Enhanced Task Execution Status

| Phase | Focus Area | Tasks | Completed | In Progress | Not Started |
|-------|------------|-------|-----------|-------------|-------------|
| Phase 0 | Infrastructure Foundation | 4 | 0 | 0 | 4 |
| Phase 1 | Qdrant + GraphQL/gRPC Setup | 8 | 0 | 0 | 8 |
| Phase 2 | Embedded AI Models | 6 | 0 | 0 | 6 |
| Phase 3 | Integration + External Model Tests | 9 | 0 | 0 | 9 |
| Phase 4 | Performance + Scalability Testing | 8 | 0 | 0 | 8 |
| Phase 5 | Monitoring + R&D Handoff | 4 | 0 | 0 | 4 |
| **TOTAL** | **Enhanced Implementation** | **39** | **0** | **0** | **39** |

---

## 📋 PRD Requirements Traceability Matrix

### **Functional Requirements (FR) Mapping:**
- **FR-VDB-001**: Vector storage and retrieval operations
- **FR-VDB-002**: Embedding generation with 4 local models
- **FR-VDB-003**: Multi-API support (REST, GraphQL, gRPC)
- **FR-VDB-004**: External AI model integration (9 models)
- **FR-VDB-005**: Real-time query processing
- **FR-VDB-006**: Metadata management and filtering
- **FR-VDB-007**: Batch processing capabilities
- **FR-VDB-008**: Collection management operations

### **Non-Functional Requirements (NFR) Mapping:**
- **NFR-PERF-001**: >10,000 vector operations per second
- **NFR-PERF-002**: <10ms average query latency
- **NFR-PERF-003**: <100ms embedding generation latency
- **NFR-PERF-004**: >80% GPU utilization efficiency
- **NFR-SCALE-001**: Support for 100M+ vectors
- **NFR-SCALE-002**: Horizontal scaling capability
- **NFR-AVAIL-001**: 99% uptime for R&D environment
- **NFR-COMPAT-001**: Multi-protocol API compatibility

---

# 🚀 PHASE 0: INFRASTRUCTURE FOUNDATION & PREREQUISITES

## Task 0.1: Hardware Verification and GPU Assessment
**PRD References:** NFR-PERF-004, NFR-SCALE-001  
**Priority:** High | **Duration:** 60 minutes | **Dependencies:** None

### Objective
Verify server hardware specifications and assess dual GPU configuration for embedded model deployment, ensuring hardware meets performance and scalability requirements.

### Success Criteria
- [ ] Intel Core i9-9900K CPU verified (8 cores, 16 threads) - **NFR-PERF-001**
- [ ] 78GB RAM availability confirmed - **NFR-SCALE-001**
- [ ] 2x NVIDIA GeForce GT 1030 GPUs detected (6GB VRAM each) - **NFR-PERF-004**
- [ ] 21.8TB total storage verified - **NFR-SCALE-001**
- [ ] Network interface eno1 configured at 192.168.10.30

### Implementation Steps
```bash
# CPU verification
lscpu | grep -E "Model name|CPU\(s\)|Thread"

# Memory verification
free -h | grep Mem

# GPU verification
lspci | grep -i nvidia
nvidia-smi -L 2>/dev/null || echo "NVIDIA drivers pending installation"

# Storage verification
lsblk -f && df -h

# Network verification
ip a show eno1
```

### Validation Commands
```bash
# Hardware assessment script
cat > /tmp/hardware_check.sh << 'EOF'
#!/bin/bash
echo "=== Hardware Verification Report ==="
echo "CPU: $(lscpu | grep 'Model name' | cut -d: -f2 | xargs)"
echo "Cores: $(nproc)"
echo "Memory: $(free -h | grep Mem | awk '{print $2}')"
echo "GPUs: $(lspci | grep -i nvidia | wc -l)"
echo "Storage: $(df -h --total | grep total | awk '{print $2}')"
echo "Network: $(ip a show eno1 | grep 'inet ' | awk '{print $2}')"
EOF
chmod +x /tmp/hardware_check.sh && /tmp/hardware_check.sh
```

---

## Task 0.2: Operating System Optimization and Updates
**PRD References:** NFR-PERF-001, NFR-AVAIL-001  
**Priority:** High | **Duration:** 75 minutes | **Dependencies:** Task 0.1

### Objective
Optimize Ubuntu 24.04.2 LTS for vector database and GPU workloads with minimal security configuration aligned with Project 1 standards.

### Success Criteria
- [ ] System packages updated to latest stable versions - **NFR-AVAIL-001**
- [ ] Kernel optimized for high-performance computing - **NFR-PERF-001**
- [ ] System limits configured for database operations - **NFR-PERF-001**
- [ ] Basic firewall configured (UFW with essential ports) - **Minimum Security**
- [ ] Network stack optimized for high-throughput - **NFR-PERF-001**

### Implementation Steps
```bash
# System updates
sudo apt update && sudo apt upgrade -y

# Install essential packages
sudo apt install -y build-essential curl wget git htop iotop

# Kernel optimization for database workloads
sudo tee -a /etc/sysctl.conf << 'EOF'
# Vector database optimizations
vm.swappiness=10
vm.dirty_ratio=15
vm.dirty_background_ratio=5
fs.file-max=2097152
net.core.rmem_max=134217728
net.core.wmem_max=134217728
net.core.netdev_max_backlog=5000
EOF

# Apply kernel parameters
sudo sysctl -p

# Configure system limits
sudo tee -a /etc/security/limits.conf << 'EOF'
# Database limits
* soft nofile 65536
* hard nofile 65536
* soft nproc 32768
* hard nproc 32768
EOF

# Basic firewall setup (minimal security)
sudo ufw --force reset
sudo ufw default deny incoming
sudo ufw default allow outgoing
sudo ufw allow ssh
sudo ufw allow 6333/tcp  # Qdrant HTTP
sudo ufw allow 6334/tcp  # Qdrant gRPC
sudo ufw allow 8000/tcp  # FastAPI Embedding Service
sudo ufw allow 8080/tcp  # GraphQL
sudo ufw --force enable
```

### Validation Commands
```bash
# Verify optimizations
sysctl vm.swappiness fs.file-max net.core.rmem_max
ulimit -n
sudo ufw status numbered
```

---

## Task 0.3: NVIDIA Driver and CUDA Installation
**PRD References:** NFR-PERF-004, FR-VDB-002  
**Priority:** High | **Duration:** 120 minutes | **Dependencies:** Task 0.2

### Objective
Install and configure NVIDIA drivers and CUDA toolkit for dual GT 1030 GPU support, enabling AI model inference capabilities.

### Success Criteria
- [ ] NVIDIA drivers 535+ installed and functional - **NFR-PERF-004**
- [ ] CUDA toolkit 12.x installed and configured - **FR-VDB-002**
- [ ] Both GPUs detected and accessible via nvidia-smi - **NFR-PERF-004**
- [ ] CUDA samples compiled and tested successfully
- [ ] GPU memory and compute capabilities verified

### Implementation Steps
```bash
# Remove conflicting drivers
sudo apt remove --purge nvidia-* -y
sudo apt autoremove -y

# Add NVIDIA repository
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update

# Install NVIDIA drivers and CUDA
sudo apt install -y nvidia-driver-535 cuda-toolkit-12-3

# Configure environment variables
echo 'export CUDA_HOME=/usr/local/cuda' | sudo tee -a /etc/environment
echo 'export PATH=$PATH:/usr/local/cuda/bin' | sudo tee -a /etc/environment
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64' | sudo tee -a /etc/environment

# Reboot required for driver activation
sudo reboot
```

### Post-Reboot Validation
```bash
# Verify NVIDIA installation
nvidia-smi
nvcc --version

# Test CUDA functionality
cd /usr/local/cuda/samples/1_Utilities/deviceQuery
sudo make
./deviceQuery

# GPU memory test
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv
```

---

## Task 0.4: Python Environment and Dependencies Setup
**PRD References:** FR-VDB-002, FR-VDB-003  
**Priority:** High | **Duration:** 90 minutes | **Dependencies:** Task 0.3

### Objective
Configure Python 3.12.3 environment with comprehensive packages for vector operations, AI model inference, and multi-protocol API support.

### Success Criteria
- [ ] Python 3.12.3 virtual environment created at /opt/citadel/env - **FR-VDB-002**
- [ ] PyTorch 2.x with CUDA support installed - **FR-VDB-002**
- [ ] AI model libraries installed (transformers, sentence-transformers) - **FR-VDB-002**
- [ ] API frameworks installed (FastAPI, GraphQL, gRPC) - **FR-VDB-003**
- [ ] Testing frameworks installed (pytest, Locust) - **Testing Enhancement**

### Implementation Steps
```bash
# Install Python 3.12
sudo apt install -y python3.12 python3.12-venv python3.12-dev

# Create virtual environment
sudo mkdir -p /opt/citadel
sudo python3.12 -m venv /opt/citadel/env
sudo chown -R agent0:agent0 /opt/citadel

# Activate environment and upgrade pip
source /opt/citadel/env/bin/activate
pip install --upgrade pip setuptools wheel

# Install core dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install AI model libraries
pip install transformers sentence-transformers accelerate

# Install vector database clients
pip install qdrant-client

# Install API frameworks
pip install fastapi uvicorn pydantic
pip install strawberry-graphql  # GraphQL
pip install grpcio grpcio-tools  # gRPC

# Install testing frameworks
pip install pytest pytest-asyncio pytest-cov
pip install locust  # Load testing
pip install httpx  # Async HTTP client for testing

# Install additional utilities
pip install numpy pandas matplotlib seaborn
pip install python-dotenv pyyaml
pip install redis psycopg2-binary

# Create requirements.txt
pip freeze > /opt/citadel/requirements.txt
```

### Validation Commands
```bash
# Verify Python environment
source /opt/citadel/env/bin/activate
python --version
pip list | grep -E "torch|transformers|qdrant|fastapi|strawberry|grpcio|pytest|locust"

# Test CUDA availability
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Devices: {torch.cuda.device_count()}')"

# Test library imports
python -c "
import transformers
import qdrant_client
import fastapi
import strawberry
import grpc
import pytest
import locust
print('All libraries imported successfully')
"
```

---

# 🏗️ PHASE 1: QDRANT VECTOR DATABASE + GRAPHQL/GRPC SETUP

## Task 1.1: Qdrant Installation and Basic Configuration
**PRD References:** FR-VDB-001, NFR-PERF-001, NFR-AVAIL-001  
**Priority:** High | **Duration:** 60 minutes | **Dependencies:** Task 0.4

### Objective
Install Qdrant 1.8+ vector database with optimized configuration for R&D environment, supporting high-performance vector operations.

### Success Criteria
- [ ] Qdrant 1.8+ installed and running - **FR-VDB-001**
- [ ] HTTP API accessible on port 6333 - **FR-VDB-001**
- [ ] gRPC API accessible on port 6334 - **FR-VDB-003**
- [ ] Service configured for auto-start - **NFR-AVAIL-001**
- [ ] Basic health monitoring enabled

### Implementation Steps
```bash
# Download and install Qdrant
cd /tmp
wget https://github.com/qdrant/qdrant/releases/download/v1.8.1/qdrant-x86_64-unknown-linux-gnu.tar.gz
tar -xzf qdrant-x86_64-unknown-linux-gnu.tar.gz

# Create Qdrant user and directories
sudo useradd -r -s /bin/false qdrant
sudo mkdir -p /opt/qdrant/{bin,config,data,logs}
sudo cp qdrant /opt/qdrant/bin/
sudo chown -R qdrant:qdrant /opt/qdrant

# Create configuration file
sudo tee /opt/qdrant/config/config.yaml << 'EOF'
log_level: INFO
storage:
  storage_path: /opt/qdrant/data
service:
  host: 0.0.0.0
  http_port: 6333
  grpc_port: 6334
  enable_cors: true
  max_request_size_mb: 32
  max_workers: 8
cluster:
  enabled: false
EOF

# Create systemd service
sudo tee /etc/systemd/system/qdrant.service << 'EOF'
[Unit]
Description=Qdrant Vector Database
After=network.target

[Service]
Type=exec
User=qdrant
Group=qdrant
ExecStart=/opt/qdrant/bin/qdrant --config-path /opt/qdrant/config/config.yaml
Restart=always
RestartSec=5
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

# Enable and start service
sudo systemctl daemon-reload
sudo systemctl enable qdrant
sudo systemctl start qdrant
```

### Validation Commands
```bash
# Service status
systemctl status qdrant

# API health check
curl http://192.168.10.30:6333/health
curl http://192.168.10.30:6333/ | jq .

# gRPC port verification
netstat -tlnp | grep 6334
```

---

## Task 1.2: Storage Configuration and Optimization
**PRD References:** NFR-SCALE-001, NFR-PERF-001  
**Priority:** High | **Duration:** 60 minutes | **Dependencies:** Task 1.1

### Objective
Configure optimized storage layout across 21.8TB capacity for maximum I/O performance and scalability.

### Success Criteria
- [ ] Primary storage on NVMe optimized for performance - **NFR-PERF-001**
- [ ] Secondary storage configured for scale - **NFR-SCALE-001**
- [ ] Backup storage configured on separate device
- [ ] I/O optimization applied for vector workloads
- [ ] Storage monitoring configured

### Implementation Steps
```bash
# Analyze storage layout
lsblk -f
df -h

# Create optimized mount points
sudo mkdir -p /opt/qdrant/data /data/vector-storage /backup/qdrant

# Configure storage optimization in fstab (if needed)
# Note: Adjust based on actual storage configuration
sudo tee -a /etc/fstab << 'EOF'
# Vector database storage optimizations
# /dev/nvme0n1p1 /opt/qdrant/data ext4 defaults,noatime,data=writeback 0 2
EOF

# Set optimal permissions
sudo chown -R qdrant:qdrant /opt/qdrant/data
sudo chown -R agent0:agent0 /data/vector-storage /backup/qdrant

# Configure I/O scheduler for SSDs
echo 'noop' | sudo tee /sys/block/nvme0n1/queue/scheduler

# Create storage monitoring script
sudo tee /opt/citadel/scripts/storage_monitor.sh << 'EOF'
#!/bin/bash
echo "=== Storage Usage Report ==="
df -h | grep -E "qdrant|vector|backup"
echo "=== I/O Statistics ==="
iostat -x 1 1 | grep -E "Device|nvme|sd"
EOF
chmod +x /opt/citadel/scripts/storage_monitor.sh
```

### Validation Commands
```bash
# Storage verification
df -h | grep -E "qdrant|vector|backup"
ls -la /opt/qdrant/data /data/vector-storage /backup/qdrant

# I/O performance test
/opt/citadel/scripts/storage_monitor.sh
```

---

## Task 1.3: Qdrant Performance Tuning
**PRD References:** NFR-PERF-001, NFR-PERF-002  
**Priority:** High | **Duration:** 75 minutes | **Dependencies:** Task 1.2

### Objective
Optimize Qdrant configuration for high-performance vector operations targeting >10K ops/sec and <10ms latency.

### Success Criteria
- [ ] Worker threads optimized for 8-core CPU - **NFR-PERF-001**
- [ ] Memory settings optimized for 78GB RAM - **NFR-PERF-001**
- [ ] HNSW parameters tuned for performance - **NFR-PERF-002**
- [ ] Request limits configured appropriately
- [ ] Performance baseline established

### Implementation Steps
```bash
# Update Qdrant configuration for performance
sudo tee /opt/qdrant/config/config.yaml << 'EOF'
log_level: INFO
storage:
  storage_path: /opt/qdrant/data
  wal_capacity_mb: 32
  wal_segments_ahead: 0
  performance:
    max_search_threads: 8
    max_optimization_threads: 4
service:
  host: 0.0.0.0
  http_port: 6333
  grpc_port: 6334
  enable_cors: true
  max_request_size_mb: 64
  max_workers: 8
  grpc_timeout_ms: 30000
hnsw_config:
  m: 16
  ef_construct: 200
  full_scan_threshold: 10000
  max_indexing_threads: 4
optimizer:
  deleted_threshold: 0.2
  vacuum_min_vector_number: 1000
  default_segment_number: 0
  max_segment_size_mb: 100
  memmap_threshold_mb: 100
  indexing_threshold_mb: 100
  flush_interval_sec: 5
  max_optimization_threads: 4
EOF

# Restart Qdrant with new configuration
sudo systemctl restart qdrant
sleep 10

# Create performance benchmark script
sudo tee /opt/citadel/scripts/qdrant_benchmark.py << 'EOF'
#!/usr/bin/env python3
import time
import numpy as np
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

def benchmark_qdrant():
    client = QdrantClient(host="192.168.10.30", port=6333)
    
    # Create test collection
    collection_name = "benchmark_test"
    try:
        client.delete_collection(collection_name)
    except:
        pass
    
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=128, distance=Distance.COSINE)
    )
    
    # Generate test vectors
    vectors = np.random.random((1000, 128)).astype(np.float32)
    points = [
        PointStruct(id=i, vector=vectors[i].tolist(), payload={"test": i})
        for i in range(1000)
    ]
    
    # Benchmark insertion
    start_time = time.time()
    client.upsert(collection_name=collection_name, points=points)
    insert_time = time.time() - start_time
    
    # Benchmark search
    query_vector = np.random.random(128).astype(np.float32)
    search_times = []
    
    for _ in range(100):
        start_time = time.time()
        client.search(
            collection_name=collection_name,
            query_vector=query_vector.tolist(),
            limit=10
        )
        search_times.append(time.time() - start_time)
    
    avg_search_time = np.mean(search_times) * 1000  # Convert to ms
    
    print(f"Insertion time for 1000 vectors: {insert_time:.2f}s")
    print(f"Average search time: {avg_search_time:.2f}ms")
    print(f"Search throughput: {1000/avg_search_time:.0f} ops/sec")
    
    # Cleanup
    client.delete_collection(collection_name)
    
    return {
        "insert_time": insert_time,
        "avg_search_time_ms": avg_search_time,
        "search_throughput": 1000/avg_search_time
    }

if __name__ == "__main__":
    benchmark_qdrant()
EOF

chmod +x /opt/citadel/scripts/qdrant_benchmark.py
```

### Validation Commands
```bash
# Verify configuration
sudo systemctl status qdrant
curl http://192.168.10.30:6333/health

# Run performance benchmark
source /opt/citadel/env/bin/activate
python /opt/citadel/scripts/qdrant_benchmark.py

# Monitor resource usage
top -p $(pgrep qdrant)
```

---

## Task 1.4: Vector Collections Creation
**PRD References:** FR-VDB-001, FR-VDB-004, FR-VDB-008  
**Priority:** High | **Duration:** 45 minutes | **Dependencies:** Task 1.3

### Objective
Create all 13 vector collections for external AI models and embedded models with optimized configurations.

### Success Criteria
- [ ] 9 external AI model collections created - **FR-VDB-004**
- [ ] 4 embedded model collections created - **FR-VDB-002**
- [ ] Appropriate vector dimensions configured - **FR-VDB-001**
- [ ] Distance metrics configured (Cosine similarity) - **FR-VDB-001**
- [ ] Metadata schemas defined - **FR-VDB-006**

### Implementation Steps
```bash
# Create collections configuration script
source /opt/citadel/env/bin/activate

cat > /opt/citadel/scripts/create_collections.py << 'EOF'
#!/usr/bin/env python3
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, CreateCollection

def create_collections():
    client = QdrantClient(host="192.168.10.30", port=6333)
    
    # External AI Model Collections (9 collections)
    external_collections = {
        "mixtral_embeddings": {"size": 4096, "description": "Mixtral-8x7B model embeddings"},
        "hermes_documents": {"size": 4096, "description": "Nous Hermes 2 document embeddings"},
        "yi34_documents": {"size": 4096, "description": "Yi-34B long document embeddings"},
        "openchat_embeddings": {"size": 4096, "description": "OpenChat 3.5 conversation embeddings"},
        "phi3_embeddings": {"size": 3072, "description": "Phi-3 Mini model embeddings"},
        "deepcoder_embeddings": {"size": 4096, "description": "DeepCoder-14B code embeddings"},
        "imp_embeddings": {"size": 4096, "description": "IMP model embeddings"},
        "deepseek_embeddings": {"size": 4096, "description": "DeepSeek model embeddings"},
        "general_embeddings": {"size": 1536, "description": "General purpose embeddings"}
    }
    
    # Embedded Model Collections (4 collections)
    embedded_collections = {
        "minilm_general": {"size": 384, "description": "all-MiniLM-L6-v2 general embeddings"},
        "phi3mini_embeddings": {"size": 3072, "description": "phi-3-mini local embeddings"},
        "e5small_embeddings": {"size": 384, "description": "e5-small model embeddings"},
        "bgebase_embeddings": {"size": 768, "description": "bge-base model embeddings"}
    }
    
    all_collections = {**external_collections, **embedded_collections}
    
    for collection_name, config in all_collections.items():
        try:
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=config["size"],
                    distance=Distance.COSINE
                )
            )
            print(f"✓ Created collection: {collection_name} (dim: {config['size']})")
        except Exception as e:
            print(f"✗ Failed to create {collection_name}: {e}")
    
    # Verify collections
    collections = client.get_collections()
    print(f"\nTotal collections created: {len(collections.collections)}")
    for collection in collections.collections:
        print(f"  - {collection.name}")

if __name__ == "__main__":
    create_collections()
EOF

chmod +x /opt/citadel/scripts/create_collections.py

# Execute collection creation
python /opt/citadel/scripts/create_collections.py
```

### Validation Commands
```bash
# Verify collections via API
curl http://192.168.10.30:6333/collections | jq '.result.collections[].name'

# Check specific collection details
curl http://192.168.10.30:6333/collections/mixtral_embeddings | jq .

# Test vector insertion
curl -X PUT http://192.168.10.30:6333/collections/minilm_general/points \
  -H "Content-Type: application/json" \
  -d '{
    "points": [
      {
        "id": 1,
        "vector": [0.1, 0.2, 0.3, 0.4],
        "payload": {"test": "sample", "model": "minilm"}
      }
    ]
  }'
```

---

## Task 1.5: GraphQL API Implementation
**PRD References:** FR-VDB-003, NFR-COMPAT-001  
**Priority:** High | **Duration:** 90 minutes | **Dependencies:** Task 1.4

### Objective
Implement GraphQL API interface for vector database operations, providing modern query capabilities and type safety.

### Success Criteria
- [ ] GraphQL server running on port 8080 - **FR-VDB-003**
- [ ] Complete schema for vector operations - **FR-VDB-003**
- [ ] Query and mutation resolvers implemented - **FR-VDB-003**
- [ ] GraphQL playground accessible - **NFR-COMPAT-001**
- [ ] Integration with Qdrant backend

### Implementation Steps
```bash
source /opt/citadel/env/bin/activate

# Create GraphQL service directory
mkdir -p /opt/citadel/services/graphql

# Create GraphQL schema and resolvers
cat > /opt/citadel/services/graphql/schema.py << 'EOF'
import strawberry
from typing import List, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, Filter, FieldCondition, MatchValue

@strawberry.type
class VectorPoint:
    id: int
    vector: List[float]
    payload: Optional[str] = None

@strawberry.type
class SearchResult:
    id: int
    score: float
    payload: Optional[str] = None

@strawberry.type
class Collection:
    name: str
    vectors_count: int
    indexed_vectors_count: int
    points_count: int

@strawberry.input
class VectorInput:
    id: int
    vector: List[float]
    payload: Optional[str] = None

@strawberry.type
class Query:
    @strawberry.field
    def collections(self) -> List[Collection]:
        client = QdrantClient(host="192.168.10.30", port=6333)
        collections = client.get_collections()
        result = []
        for collection in collections.collections:
            info = client.get_collection(collection.name)
            result.append(Collection(
                name=collection.name,
                vectors_count=info.vectors_count or 0,
                indexed_vectors_count=info.indexed_vectors_count or 0,
                points_count=info.points_count or 0
            ))
        return result
    
    @strawberry.field
    def search_vectors(
        self,
        collection_name: str,
        query_vector: List[float],
        limit: int = 10,
        score_threshold: Optional[float] = None
    ) -> List[SearchResult]:
        client = QdrantClient(host="192.168.10.30", port=6333)
        results = client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit,
            score_threshold=score_threshold
        )
        return [
            SearchResult(
                id=result.id,
                score=result.score,
                payload=str(result.payload) if result.payload else None
            )
            for result in results
        ]

@strawberry.type
class Mutation:
    @strawberry.mutation
    def upsert_vectors(
        self,
        collection_name: str,
        points: List[VectorInput]
    ) -> bool:
        client = QdrantClient(host="192.168.10.30", port=6333)
        qdrant_points = [
            PointStruct(
                id=point.id,
                vector=point.vector,
                payload={"data": point.payload} if point.payload else {}
            )
            for point in points
        ]
        client.upsert(collection_name=collection_name, points=qdrant_points)
        return True
    
    @strawberry.mutation
    def delete_vectors(
        self,
        collection_name: str,
        point_ids: List[int]
    ) -> bool:
        client = QdrantClient(host="192.168.10.30", port=6333)
        client.delete(collection_name=collection_name, points_selector=point_ids)
        return True

schema = strawberry.Schema(query=Query, mutation=Mutation)
EOF

# Create GraphQL server
cat > /opt/citadel/services/graphql/server.py << 'EOF'
import uvicorn
from fastapi import FastAPI
from strawberry.fastapi import GraphQLRouter
from schema import schema

app = FastAPI(title="Citadel Vector Database GraphQL API")

graphql_app = GraphQLRouter(schema, graphiql=True)
app.include_router(graphql_app, prefix="/graphql")

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "graphql-api"}

if __name__ == "__main__":
    uvicorn.run(
        "server:app",
        host="0.0.0.0",
        port=8080,
        reload=False,
        workers=1
    )
EOF

# Create systemd service for GraphQL
sudo tee /etc/systemd/system/citadel-graphql.service << 'EOF'
[Unit]
Description=Citadel GraphQL API
After=network.target qdrant.service

[Service]
Type=exec
User=agent0
Group=agent0
WorkingDirectory=/opt/citadel/services/graphql
Environment=PATH=/opt/citadel/env/bin
ExecStart=/opt/citadel/env/bin/python server.py
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Enable and start GraphQL service
sudo systemctl daemon-reload
sudo systemctl enable citadel-graphql
sudo systemctl start citadel-graphql
```

### Validation Commands
```bash
# Service status
systemctl status citadel-graphql

# Health check
curl http://192.168.10.30:8080/health

# GraphQL playground access
curl http://192.168.10.30:8080/graphql

# Test GraphQL query
curl -X POST http://192.168.10.30:8080/graphql \
  -H "Content-Type: application/json" \
  -d '{"query": "{ collections { name pointsCount } }"}'
```

---

## Task 1.6: gRPC API Implementation
**PRD References:** FR-VDB-003, NFR-COMPAT-001  
**Priority:** High | **Duration:** 105 minutes | **Dependencies:** Task 1.5

### Objective
Implement gRPC API interface for high-performance vector operations with protocol buffer definitions.

### Success Criteria
- [ ] gRPC server running on port 8081 - **FR-VDB-003**
- [ ] Protocol buffer definitions for vector operations - **FR-VDB-003**
- [ ] Service implementations for all vector operations - **FR-VDB-003**
- [ ] High-performance binary protocol support - **NFR-PERF-001**
- [ ] Client SDK generation capability

### Implementation Steps
```bash
source /opt/citadel/env/bin/activate

# Create gRPC service directory
mkdir -p /opt/citadel/services/grpc

# Create protocol buffer definitions
cat > /opt/citadel/services/grpc/vector_service.proto << 'EOF'
syntax = "proto3";

package citadel.vector;

service VectorService {
  rpc GetCollections(GetCollectionsRequest) returns (GetCollectionsResponse);
  rpc SearchVectors(SearchVectorsRequest) returns (SearchVectorsResponse);
  rpc UpsertVectors(UpsertVectorsRequest) returns (UpsertVectorsResponse);
  rpc DeleteVectors(DeleteVectorsRequest) returns (DeleteVectorsResponse);
  rpc GetCollectionInfo(GetCollectionInfoRequest) returns (GetCollectionInfoResponse);
}

message GetCollectionsRequest {}

message GetCollectionsResponse {
  repeated CollectionInfo collections = 1;
}

message CollectionInfo {
  string name = 1;
  int64 vectors_count = 2;
  int64 indexed_vectors_count = 3;
  int64 points_count = 4;
}

message SearchVectorsRequest {
  string collection_name = 1;
  repeated float query_vector = 2;
  int32 limit = 3;
  optional float score_threshold = 4;
}

message SearchVectorsResponse {
  repeated SearchResult results = 1;
}

message SearchResult {
  int64 id = 1;
  float score = 2;
  string payload = 3;
}

message UpsertVectorsRequest {
  string collection_name = 1;
  repeated VectorPoint points = 2;
}

message UpsertVectorsResponse {
  bool success = 1;
  string message = 2;
}

message VectorPoint {
  int64 id = 1;
  repeated float vector = 2;
  string payload = 3;
}

message DeleteVectorsRequest {
  string collection_name = 1;
  repeated int64 point_ids = 2;
}

message DeleteVectorsResponse {
  bool success = 1;
  string message = 2;
}

message GetCollectionInfoRequest {
  string collection_name = 1;
}

message GetCollectionInfoResponse {
  CollectionInfo collection = 1;
}
EOF

# Generate Python gRPC code
cd /opt/citadel/services/grpc
python -m grpc_tools.protoc \
  --python_out=. \
  --grpc_python_out=. \
  --proto_path=. \
  vector_service.proto

# Create gRPC server implementation
cat > /opt/citadel/services/grpc/server.py << 'EOF'
import grpc
from concurrent import futures
import vector_service_pb2
import vector_service_pb2_grpc
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
import json

class VectorServiceImpl(vector_service_pb2_grpc.VectorServiceServicer):
    def __init__(self):
        self.client = QdrantClient(host="192.168.10.30", port=6333)
    
    def GetCollections(self, request, context):
        try:
            collections = self.client.get_collections()
            response = vector_service_pb2.GetCollectionsResponse()
            
            for collection in collections.collections:
                info = self.client.get_collection(collection.name)
                collection_info = vector_service_pb2.CollectionInfo(
                    name=collection.name,
                    vectors_count=info.vectors_count or 0,
                    indexed_vectors_count=info.indexed_vectors_count or 0,
                    points_count=info.points_count or 0
                )
                response.collections.append(collection_info)
            
            return response
        except Exception as e:
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return vector_service_pb2.GetCollectionsResponse()
    
    def SearchVectors(self, request, context):
        try:
            results = self.client.search(
                collection_name=request.collection_name,
                query_vector=list(request.query_vector),
                limit=request.limit,
                score_threshold=request.score_threshold if request.HasField('score_threshold') else None
            )
            
            response = vector_service_pb2.SearchVectorsResponse()
            for result in results:
                search_result = vector_service_pb2.SearchResult(
                    id=result.id,
                    score=result.score,
                    payload=json.dumps(result.payload) if result.payload else ""
                )
                response.results.append(search_result)
            
            return response
        except Exception as e:
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return vector_service_pb2.SearchVectorsResponse()
    
    def UpsertVectors(self, request, context):
        try:
            points = [
                PointStruct(
                    id=point.id,
                    vector=list(point.vector),
                    payload=json.loads(point.payload) if point.payload else {}
                )
                for point in request.points
            ]
            
            self.client.upsert(
                collection_name=request.collection_name,
                points=points
            )
            
            return vector_service_pb2.UpsertVectorsResponse(
                success=True,
                message="Vectors upserted successfully"
            )
        except Exception as e:
            return vector_service_pb2.UpsertVectorsResponse(
                success=False,
                message=str(e)
            )
    
    def DeleteVectors(self, request, context):
        try:
            self.client.delete(
                collection_name=request.collection_name,
                points_selector=list(request.point_ids)
            )
            
            return vector_service_pb2.DeleteVectorsResponse(
                success=True,
                message="Vectors deleted successfully"
            )
        except Exception as e:
            return vector_service_pb2.DeleteVectorsResponse(
                success=False,
                message=str(e)
            )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    vector_service_pb2_grpc.add_VectorServiceServicer_to_server(
        VectorServiceImpl(), server
    )
    
    listen_addr = '0.0.0.0:8081'
    server.add_insecure_port(listen_addr)
    
    print(f"Starting gRPC server on {listen_addr}")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    serve()
EOF

# Create systemd service for gRPC
sudo tee /etc/systemd/system/citadel-grpc.service << 'EOF'
[Unit]
Description=Citadel gRPC API
After=network.target qdrant.service

[Service]
Type=exec
User=agent0
Group=agent0
WorkingDirectory=/opt/citadel/services/grpc
Environment=PATH=/opt/citadel/env/bin
ExecStart=/opt/citadel/env/bin/python server.py
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

# Enable and start gRPC service
sudo systemctl daemon-reload
sudo systemctl enable citadel-grpc
sudo systemctl start citadel-grpc
```

### Validation Commands
```bash
# Service status
systemctl status citadel-grpc

# Port verification
netstat -tlnp | grep 8081

# Create gRPC client test
cat > /tmp/test_grpc_client.py << 'EOF'
import grpc
import sys
sys.path.append('/opt/citadel/services/grpc')
import vector_service_pb2
import vector_service_pb2_grpc

def test_grpc_client():
    with grpc.insecure_channel('192.168.10.30:8081') as channel:
        stub = vector_service_pb2_grpc.VectorServiceStub(channel)
        
        # Test GetCollections
        request = vector_service_pb2.GetCollectionsRequest()
        response = stub.GetCollections(request)
        
        print(f"Collections found: {len(response.collections)}")
        for collection in response.collections:
            print(f"  - {collection.name}: {collection.points_count} points")

if __name__ == "__main__":
    test_grpc_client()
EOF

source /opt/citadel/env/bin/activate
python /tmp/test_grpc_client.py
```

---

## Task 1.7: Basic Backup Configuration
**PRD References:** NFR-AVAIL-001  
**Priority:** Medium | **Duration:** 60 minutes | **Dependencies:** Task 1.6

### Objective
Configure basic backup procedures for vector data with simple recovery capabilities for R&D environment.

### Success Criteria
- [ ] Automated backup script created - **NFR-AVAIL-001**
- [ ] Backup location configured on separate storage
- [ ] Basic recovery procedure documented
- [ ] Backup monitoring implemented
- [ ] Manual backup test completed

### Implementation Steps
```bash
# Create backup script
sudo tee /opt/citadel/scripts/backup_qdrant.sh << 'EOF'
#!/bin/bash
set -e

BACKUP_DIR="/backup/qdrant"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/qdrant_backup_$TIMESTAMP"

echo "Starting Qdrant backup at $(date)"

# Create backup directory
mkdir -p "$BACKUP_PATH"

# Create snapshot via API
curl -X POST "http://192.168.10.30:6333/snapshots" \
  -H "Content-Type: application/json" \
  -d '{"snapshot_name": "backup_'$TIMESTAMP'"}' > "$BACKUP_PATH/snapshot_response.json"

# Copy data directory
cp -r /opt/qdrant/data "$BACKUP_PATH/"

# Copy configuration
cp -r /opt/qdrant/config "$BACKUP_PATH/"

# Create backup metadata
cat > "$BACKUP_PATH/backup_metadata.json" << EOL
{
  "timestamp": "$TIMESTAMP",
  "backup_type": "full",
  "qdrant_version": "1.8.1",
  "collections": $(curl -s http://192.168.10.30:6333/collections | jq '.result.collections | length'),
  "total_points": $(curl -s http://192.168.10.30:6333/cluster | jq '.result.raft_info.commit')
}
EOL

# Compress backup
cd "$BACKUP_DIR"
tar -czf "qdrant_backup_$TIMESTAMP.tar.gz" "qdrant_backup_$TIMESTAMP"
rm -rf "qdrant_backup_$TIMESTAMP"

# Cleanup old backups (keep last 7 days)
find "$BACKUP_DIR" -name "qdrant_backup_*.tar.gz" -mtime +7 -delete

echo "Backup completed: qdrant_backup_$TIMESTAMP.tar.gz"
EOF

chmod +x /opt/citadel/scripts/backup_qdrant.sh

# Create restore script
sudo tee /opt/citadel/scripts/restore_qdrant.sh << 'EOF'
#!/bin/bash
set -e

if [ $# -ne 1 ]; then
    echo "Usage: $0 <backup_file.tar.gz>"
    exit 1
fi

BACKUP_FILE="$1"
RESTORE_DIR="/tmp/qdrant_restore_$(date +%s)"

echo "Starting Qdrant restore from $BACKUP_FILE"

# Stop Qdrant service
sudo systemctl stop qdrant

# Extract backup
mkdir -p "$RESTORE_DIR"
tar -xzf "$BACKUP_FILE" -C "$RESTORE_DIR"

# Find backup directory
BACKUP_CONTENT=$(find "$RESTORE_DIR" -name "qdrant_backup_*" -type d | head -1)

# Restore data
sudo rm -rf /opt/qdrant/data/*
sudo cp -r "$BACKUP_CONTENT/data/"* /opt/qdrant/data/
sudo chown -R qdrant:qdrant /opt/qdrant/data

# Start Qdrant service
sudo systemctl start qdrant

# Wait for service to be ready
sleep 10

# Verify restore
curl -f http://192.168.10.30:6333/health

echo "Restore completed successfully"

# Cleanup
rm -rf "$RESTORE_DIR"
EOF

chmod +x /opt/citadel/scripts/restore_qdrant.sh

# Create backup monitoring script
sudo tee /opt/citadel/scripts/check_backup_status.sh << 'EOF'
#!/bin/bash

BACKUP_DIR="/backup/qdrant"
LATEST_BACKUP=$(find "$BACKUP_DIR" -name "qdrant_backup_*.tar.gz" -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -d' ' -f2-)

if [ -z "$LATEST_BACKUP" ]; then
    echo "ERROR: No backups found"
    exit 1
fi

BACKUP_AGE=$(( $(date +%s) - $(stat -c %Y "$LATEST_BACKUP") ))
BACKUP_AGE_HOURS=$(( BACKUP_AGE / 3600 ))

echo "Latest backup: $(basename "$LATEST_BACKUP")"
echo "Backup age: ${BACKUP_AGE_HOURS} hours"
echo "Backup size: $(du -h "$LATEST_BACKUP" | cut -f1)"

if [ $BACKUP_AGE_HOURS -gt 25 ]; then
    echo "WARNING: Backup is older than 25 hours"
    exit 1
fi

echo "Backup status: OK"
EOF

chmod +x /opt/citadel/scripts/check_backup_status.sh

# Schedule daily backups
echo "0 2 * * * /opt/citadel/scripts/backup_qdrant.sh" | sudo crontab -u root -
```

### Validation Commands
```bash
# Test manual backup
sudo /opt/citadel/scripts/backup_qdrant.sh

# Verify backup files
ls -la /backup/qdrant/

# Test backup monitoring
/opt/citadel/scripts/check_backup_status.sh

# Verify cron job
sudo crontab -l
```

---

## Task 1.8: API Integration Testing Framework
**PRD References:** FR-VDB-003, NFR-COMPAT-001  
**Priority:** High | **Duration:** 75 minutes | **Dependencies:** Task 1.7

### Objective
Implement comprehensive testing framework for REST, GraphQL, and gRPC APIs using pytest with automated validation.

### Success Criteria
- [ ] pytest test suite for all API endpoints - **FR-VDB-003**
- [ ] Automated API compatibility testing - **NFR-COMPAT-001**
- [ ] Test data generation and cleanup
- [ ] Continuous integration ready tests
- [ ] Performance regression testing

### Implementation Steps
```bash
source /opt/citadel/env/bin/activate

# Create test directory structure
mkdir -p /opt/citadel/tests/{unit,integration,performance}

# Create pytest configuration
cat > /opt/citadel/tests/pytest.ini << 'EOF'
[tool:pytest]
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    --cov=/opt/citadel/services
    --cov-report=html
    --cov-report=term-missing
markers =
    unit: Unit tests
    integration: Integration tests
    performance: Performance tests
    slow: Slow running tests
EOF

# Create REST API tests
cat > /opt/citadel/tests/integration/test_rest_api.py << 'EOF'
import pytest
import httpx
import numpy as np
from typing import List, Dict, Any

class TestRestAPI:
    base_url = "http://192.168.10.30:6333"
    
    @pytest.fixture
    def client(self):
        return httpx.Client(base_url=self.base_url)
    
    @pytest.fixture
    def test_collection_name(self):
        return "test_collection_rest"
    
    @pytest.fixture
    def test_vectors(self):
        return [
            {"id": 1, "vector": np.random.random(128).tolist(), "payload": {"test": "data1"}},
            {"id": 2, "vector": np.random.random(128).tolist(), "payload": {"test": "data2"}},
            {"id": 3, "vector": np.random.random(128).tolist(), "payload": {"test": "data3"}}
        ]
    
    def test_health_check(self, client):
        """Test API health endpoint"""
        response = client.get("/health")
        assert response.status_code == 200
        assert response.json()["status"] == "ok"
    
    def test_create_collection(self, client, test_collection_name):
        """Test collection creation"""
        # Clean up if exists
        client.delete(f"/collections/{test_collection_name}")
        
        payload = {
            "vectors": {
                "size": 128,
                "distance": "Cosine"
            }
        }
        response = client.put(f"/collections/{test_collection_name}", json=payload)
        assert response.status_code == 200
    
    def test_upsert_vectors(self, client, test_collection_name, test_vectors):
        """Test vector insertion"""
        payload = {"points": test_vectors}
        response = client.put(f"/collections/{test_collection_name}/points", json=payload)
        assert response.status_code == 200
    
    def test_search_vectors(self, client, test_collection_name, test_vectors):
        """Test vector search"""
        query_vector = np.random.random(128).tolist()
        payload = {
            "vector": query_vector,
            "limit": 3,
            "with_payload": True
        }
        response = client.post(f"/collections/{test_collection_name}/points/search", json=payload)
        assert response.status_code == 200
        results = response.json()["result"]
        assert len(results) <= 3
        assert all("score" in result for result in results)
    
    def test_get_collections(self, client):
        """Test collections listing"""
        response = client.get("/collections")
        assert response.status_code == 200
        collections = response.json()["result"]["collections"]
        assert isinstance(collections, list)
    
    def test_delete_vectors(self, client, test_collection_name):
        """Test vector deletion"""
        payload = {"points": [1, 2]}
        response = client.post(f"/collections/{test_collection_name}/points/delete", json=payload)
        assert response.status_code == 200
    
    def test_cleanup_collection(self, client, test_collection_name):
        """Clean up test collection"""
        response = client.delete(f"/collections/{test_collection_name}")
        assert response.status_code == 200
EOF

# Create GraphQL API tests
cat > /opt/citadel/tests/integration/test_graphql_api.py << 'EOF'
import pytest
import httpx
import json

class TestGraphQLAPI:
    base_url = "http://192.168.10.30:8080"
    
    @pytest.fixture
    def client(self):
        return httpx.Client(base_url=self.base_url)
    
    def test_health_check(self, client):
        """Test GraphQL service health"""
        response = client.get("/health")
        assert response.status_code == 200
        assert response.json()["status"] == "healthy"
    
    def test_collections_query(self, client):
        """Test GraphQL collections query"""
        query = """
        query {
            collections {
                name
                pointsCount
                vectorsCount
            }
        }
        """
        response = client.post("/graphql", json={"query": query})
        assert response.status_code == 200
        data = response.json()
        assert "data" in data
        assert "collections" in data["data"]
    
    def test_search_vectors_query(self, client):
        """Test GraphQL vector search"""
        # First ensure we have a collection with data
        query = """
        query {
            searchVectors(
                collectionName: "minilm_general",
                queryVector: [0.1, 0.2, 0.3, 0.4],
                limit: 5
            ) {
                id
                score
                payload
            }
        }
        """
        response = client.post("/graphql", json={"query": query})
        assert response.status_code == 200
        data = response.json()
        assert "data" in data
    
    def test_upsert_vectors_mutation(self, client):
        """Test GraphQL vector upsert mutation"""
        mutation = """
        mutation {
            upsertVectors(
                collectionName: "minilm_general",
                points: [
                    {
                        id: 999,
                        vector: [0.1, 0.2, 0.3, 0.4],
                        payload: "test_graphql"
                    }
                ]
            )
        }
        """
        response = client.post("/graphql", json={"query": mutation})
        assert response.status_code == 200
        data = response.json()
        assert data["data"]["upsertVectors"] == True
    
    def test_delete_vectors_mutation(self, client):
        """Test GraphQL vector deletion mutation"""
        mutation = """
        mutation {
            deleteVectors(
                collectionName: "minilm_general",
                pointIds: [999]
            )
        }
        """
        response = client.post("/graphql", json={"query": mutation})
        assert response.status_code == 200
        data = response.json()
        assert data["data"]["deleteVectors"] == True
EOF

# Create gRPC API tests
cat > /opt/citadel/tests/integration/test_grpc_api.py << 'EOF'
import pytest
import grpc
import sys
sys.path.append('/opt/citadel/services/grpc')
import vector_service_pb2
import vector_service_pb2_grpc

class TestGRPCAPI:
    
    @pytest.fixture
    def grpc_channel(self):
        channel = grpc.insecure_channel('192.168.10.30:8081')
        yield channel
        channel.close()
    
    @pytest.fixture
    def grpc_stub(self, grpc_channel):
        return vector_service_pb2_grpc.VectorServiceStub(grpc_channel)
    
    def test_get_collections(self, grpc_stub):
        """Test gRPC GetCollections"""
        request = vector_service_pb2.GetCollectionsRequest()
        response = grpc_stub.GetCollections(request)
        assert len(response.collections) > 0
        for collection in response.collections:
            assert collection.name
            assert collection.points_count >= 0
    
    def test_upsert_vectors(self, grpc_stub):
        """Test gRPC UpsertVectors"""
        request = vector_service_pb2.UpsertVectorsRequest(
            collection_name="minilm_general",
            points=[
                vector_service_pb2.VectorPoint(
                    id=998,
                    vector=[0.1, 0.2, 0.3, 0.4],
                    payload='{"test": "grpc"}'
                )
            ]
        )
        response = grpc_stub.UpsertVectors(request)
        assert response.success == True
    
    def test_search_vectors(self, grpc_stub):
        """Test gRPC SearchVectors"""
        request = vector_service_pb2.SearchVectorsRequest(
            collection_name="minilm_general",
            query_vector=[0.1, 0.2, 0.3, 0.4],
            limit=5
        )
        response = grpc_stub.SearchVectors(request)
        assert len(response.results) >= 0
        for result in response.results:
            assert result.score >= 0
            assert result.id >= 0
    
    def test_delete_vectors(self, grpc_stub):
        """Test gRPC DeleteVectors"""
        request = vector_service_pb2.DeleteVectorsRequest(
            collection_name="minilm_general",
            point_ids=[998]
        )
        response = grpc_stub.DeleteVectors(request)
        assert response.success == True
    
    def test_get_collection_info(self, grpc_stub):
        """Test gRPC GetCollectionInfo"""
        request = vector_service_pb2.GetCollectionInfoRequest(
            collection_name="minilm_general"
        )
        response = grpc_stub.GetCollectionInfo(request)
        assert response.collection.name == "minilm_general"
        assert response.collection.points_count >= 0
EOF

# Create test runner script
cat > /opt/citadel/tests/run_tests.sh << 'EOF'
#!/bin/bash
set -e

echo "=== Running Citadel Vector Database API Tests ==="

# Activate virtual environment
source /opt/citadel/env/bin/activate

# Wait for services to be ready
echo "Waiting for services to be ready..."
sleep 5

# Run tests with coverage
pytest /opt/citadel/tests/integration/ \
    --cov=/opt/citadel/services \
    --cov-report=html:/opt/citadel/tests/coverage_html \
    --cov-report=term-missing \
    --junit-xml=/opt/citadel/tests/test_results.xml \
    -v

echo "=== Test Results ==="
echo "HTML Coverage Report: /opt/citadel/tests/coverage_html/index.html"
echo "JUnit XML Report: /opt/citadel/tests/test_results.xml"
EOF

chmod +x /opt/citadel/tests/run_tests.sh
```

### Validation Commands
```bash
# Run the complete test suite
/opt/citadel/tests/run_tests.sh

# Run specific test categories
source /opt/citadel/env/bin/activate
cd /opt/citadel/tests

# REST API tests only
pytest integration/test_rest_api.py -v

# GraphQL API tests only
pytest integration/test_graphql_api.py -v

# gRPC API tests only
pytest integration/test_grpc_api.py -v

# View coverage report
ls -la coverage_html/
```

---

# ⚙️ PHASE 2: EMBEDDED AI MODELS INSTALLATION

## Task 2.1: Model Repository Setup and Download
**PRD References:** FR-VDB-002, NFR-PERF-004  
**Priority:** High | **Duration:** 120 minutes | **Dependencies:** Task 1.8

### Objective
Download and organize all 4 embedded AI models for local deployment with GPU optimization.

### Success Criteria
- [ ] all-MiniLM-L6-v2 model downloaded and verified - **FR-VDB-002**
- [ ] phi-3-mini model downloaded and verified - **FR-VDB-002**
- [ ] e5-small model downloaded and verified - **FR-VDB-002**
- [ ] bge-base model downloaded and verified - **FR-VDB-002**
- [ ] Model integrity verified with checksums
- [ ] Models optimized for GPU deployment - **NFR-PERF-004**

### Implementation Steps
```bash
source /opt/citadel/env/bin/activate

# Create model directory structure
sudo mkdir -p /opt/models/{all-MiniLM-L6-v2,phi-3-mini,e5-small,bge-base}
sudo chown -R agent0:agent0 /opt/models

# Create model download script
cat > /opt/citadel/scripts/download_models.py << 'EOF'
#!/usr/bin/env python3
import os
import hashlib
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import torch

def verify_model_integrity(model_path, expected_files):
    """Verify model files exist and are valid"""
    for file_name in expected_files:
        file_path = os.path.join(model_path, file_name)
        if not os.path.exists(file_path):
            return False, f"Missing file: {file_name}"
    return True, "Model integrity verified"

def download_sentence_transformer_model(model_name, save_path):
    """Download and save SentenceTransformer model"""
    print(f"Downloading {model_name}...")
    model = SentenceTransformer(model_name)
    model.save(save_path)
    
    # Test model loading
    test_model = SentenceTransformer(save_path)
    test_embedding = test_model.encode("test sentence")
    print(f"✓ {model_name} downloaded and verified (embedding dim: {len(test_embedding)})")
    return True

def download_transformers_model(model_name, save_path):
    """Download and save Transformers model"""
    print(f"Downloading {model_name}...")
    
    # Download tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # Save to local directory
    tokenizer.save_pretrained(save_path)
    model.save_pretrained(save_path)
    
    # Test model loading
    test_tokenizer = AutoTokenizer.from_pretrained(save_path)
    test_model = AutoModel.from_pretrained(save_path)
    
    # Test inference
    inputs = test_tokenizer("test sentence", return_tensors="pt")
    with torch.no_grad():
        outputs = test_model(**inputs)
    
    print(f"✓ {model_name} downloaded and verified (hidden size: {outputs.last_hidden_state.shape[-1]})")
    return True

def main():
    models_config = {
        "all-MiniLM-L6-v2": {
            "hf_name": "sentence-transformers/all-MiniLM-L6-v2",
            "local_path": "/opt/models/all-MiniLM-L6-v2",
            "type": "sentence_transformer",
            "expected_dim": 384
        },
        "phi-3-mini": {
            "hf_name": "microsoft/Phi-3-mini-4k-instruct",
            "local_path": "/opt/models/phi-3-mini",
            "type": "transformers",
            "expected_dim": 3072
        },
        "e5-small": {
            "hf_name": "intfloat/e5-small-v2",
            "local_path": "/opt/models/e5-small",
            "type": "sentence_transformer",
            "expected_dim": 384
        },
        "bge-base": {
            "hf_name": "BAAI/bge-base-en-v1.5",
            "local_path": "/opt/models/bge-base",
            "type": "sentence_transformer",
            "expected_dim": 768
        }
    }
    
    print("=== Starting Model Downloads ===")
    
    for model_name, config in models_config.items():
        try:
            if config["type"] == "sentence_transformer":
                download_sentence_transformer_model(config["hf_name"], config["local_path"])
            else:
                download_transformers_model(config["hf_name"], config["local_path"])
                
            # Create model metadata
            metadata = {
                "model_name": model_name,
                "hf_name": config["hf_name"],
                "local_path": config["local_path"],
                "type": config["type"],
                "expected_dim": config["expected_dim"],
                "download_date": str(torch.utils.data.get_worker_info()),
                "gpu_compatible": True
            }
            
            import json
            with open(f"{config['local_path']}/model_metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2)
                
        except Exception as e:
            print(f"✗ Failed to download {model_name}: {e}")
            return False
    
    print("=== All Models Downloaded Successfully ===")
    return True

if __name__ == "__main__":
    main()
EOF

chmod +x /opt/citadel/scripts/download_models.py

# Execute model downloads
python /opt/citadel/scripts/download_models.py

# Create model verification script
cat > /opt/citadel/scripts/verify_models.py << 'EOF'
#!/usr/bin/env python3
import os
import json
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel

def verify_all_models():
    """Verify all downloaded models"""
    models_dir = "/opt/models"
    model_names = ["all-MiniLM-L6-v2", "phi-3-mini", "e5-small", "bge-base"]
    
    print("=== Model Verification Report ===")
    
    for model_name in model_names:
        model_path = os.path.join(models_dir, model_name)
        metadata_path = os.path.join(model_path, "model_metadata.json")
        
        if not os.path.exists(model_path):
            print(f"✗ {model_name}: Directory not found")
            continue
            
        if not os.path.exists(metadata_path):
            print(f"✗ {model_name}: Metadata not found")
            continue
            
        try:
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # Test model loading based on type
            if metadata["type"] == "sentence_transformer":
                model = SentenceTransformer(model_path)
                test_embedding = model.encode("test sentence")
                actual_dim = len(test_embedding)
            else:
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = AutoModel.from_pretrained(model_path)
                inputs = tokenizer("test sentence", return_tensors="pt")
                with torch.no_grad():
                    outputs = model(**inputs)
                actual_dim = outputs.last_hidden_state.shape[-1]
            
            expected_dim = metadata["expected_dim"]
            
            if actual_dim == expected_dim:
                print(f"✓ {model_name}: OK (dim: {actual_dim})")
            else:
                print(f"⚠ {model_name}: Dimension mismatch (expected: {expected_dim}, actual: {actual_dim})")
                
        except Exception as e:
            print(f"✗ {model_name}: Error - {e}")
    
    # Check GPU availability
    print(f"\nGPU Status:")
    print(f"CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU Count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")

if __name__ == "__main__":
    verify_all_models()
EOF

chmod +x /opt/citadel/scripts/verify_models.py
```

### Validation Commands
```bash
# Verify model downloads
python /opt/citadel/scripts/verify_models.py

# Check model directory structure
find /opt/models -type f -name "*.json" -exec cat {} \;

# Check model sizes
du -sh /opt/models/*

# Test GPU access
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
```

---

## Task 2.2: GPU Memory Allocation and Model Loading
**PRD References:** NFR-PERF-004, FR-VDB-002  
**Priority:** High | **Duration:** 105 minutes | **Dependencies:** Task 2.1

### Objective
Configure optimal GPU memory allocation and implement dynamic model loading strategy across dual GPUs.

### Success Criteria
- [ ] GPU 0 configured for all-MiniLM-L6-v2 and phi-3-mini - **NFR-PERF-004**
- [ ] GPU 1 configured for e5-small and bge-base - **NFR-PERF-004**
- [ ] Memory allocation optimized (80% utilization target) - **NFR-PERF-004**
- [ ] Dynamic model loading implemented - **FR-VDB-002**
- [ ] GPU memory monitoring configured
- [ ] Model switching capability implemented

### Implementation Steps
```bash
source /opt/citadel/env/bin/activate

# Create GPU memory management module
cat > /opt/citadel/services/gpu_manager.py << 'EOF'
#!/usr/bin/env python3
import torch
import psutil
import threading
import time
from typing import Dict, Optional, Any
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GPUMemoryManager:
    def __init__(self):
        self.models = {}
        self.gpu_allocation = {
            0: ["all-MiniLM-L6-v2", "phi-3-mini"],
            1: ["e5-small", "bge-base"]
        }
        self.model_configs = {
            "all-MiniLM-L6-v2": {
                "path": "/opt/models/all-MiniLM-L6-v2",
                "type": "sentence_transformer",
                "gpu": 0,
                "loaded": False
            },
            "phi-3-mini": {
                "path": "/opt/models/phi-3-mini",
                "type": "transformers",
                "gpu": 0,
                "loaded": False
            },
            "e5-small": {
                "path": "/opt/models/e5-small",
                "type": "sentence_transformer",
                "gpu": 1,
                "loaded": False
            },
            "bge-base": {
                "path": "/opt/models/bge-base",
                "type": "sentence_transformer",
                "gpu": 1,
                "loaded": False
            }
        }
        self.lock = threading.Lock()
        
    def get_gpu_memory_info(self) -> Dict[int, Dict[str, float]]:
        """Get memory information for all GPUs"""
        gpu_info = {}
        for i in range(torch.cuda.device_count()):
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved(i) / 1024**3   # GB
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
            
            gpu_info[i] = {
                "allocated": memory_allocated,
                "reserved": memory_reserved,
                "total": memory_total,
                "free": memory_total - memory_reserved,
                "utilization": (memory_reserved / memory_total) * 100
            }
        return gpu_info
    
    def load_model(self, model_name: str) -> bool:
        """Load a model onto its designated GPU"""
        with self.lock:
            if model_name not in self.model_configs:
                logger.error(f"Unknown model: {model_name}")
                return False
            
            config = self.model_configs[model_name]
            gpu_id = config["gpu"]
            
            try:
                # Set GPU device
                device = f"cuda:{gpu_id}"
                
                if config["type"] == "sentence_transformer":
                    model = SentenceTransformer(config["path"], device=device)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(config["path"])
                    model = AutoModel.from_pretrained(config["path"]).to(device)
                    self.models[f"{model_name}_tokenizer"] = tokenizer
                
                self.models[model_name] = model
                config["loaded"] = True
                
                logger.info(f"✓ Loaded {model_name} on GPU {gpu_id}")
                return True
                
            except Exception as e:
                logger.error(f"✗ Failed to load {model_name}: {e}")
                return False
    
    def unload_model(self, model_name: str) -> bool:
        """Unload a model from GPU memory"""
        with self.lock:
            if model_name in self.models:
                del self.models[model_name]
                if f"{model_name}_tokenizer" in self.models:
                    del self.models[f"{model_name}_tokenizer"]
                
                self.model_configs[model_name]["loaded"] = False
                
                # Force garbage collection
                torch.cuda.empty_cache()
                
                logger.info(f"✓ Unloaded {model_name}")
                return True
            return False
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """Get a loaded model"""
        return self.models.get(model_name)
    
    def get_tokenizer(self, model_name: str) -> Optional[Any]:
        """Get tokenizer for transformers models"""
        return self.models.get(f"{model_name}_tokenizer")
    
    def load_all_models(self) -> Dict[str, bool]:
        """Load all models onto their designated GPUs"""
        results = {}
        for model_name in self.model_configs:
            results[model_name] = self.load_model(model_name)
        return results
    
    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive status of GPU and model allocation"""
        gpu_info = self.get_gpu_memory_info()
        model_status = {}
        
        for model_name, config in self.model_configs.items():
            model_status[model_name] = {
                "loaded": config["loaded"],
                "gpu": config["gpu"],
                "type": config["type"]
            }
        
        return {
            "gpu_memory": gpu_info,
            "models": model_status,
            "total_models": len(self.model_configs),
            "loaded_models": sum(1 for config in self.model_configs.values() if config["loaded"])
        }

# Global GPU manager instance
gpu_manager = GPUMemoryManager()
EOF

# Create GPU monitoring script
cat > /opt/citadel/scripts/gpu_monitor.py << 'EOF'
#!/usr/bin/env python3
import sys
sys.path.append('/opt/citadel/services')
from gpu_manager import gpu_manager
import json
import time

def monitor_gpu_usage():
    """Monitor GPU usage and model allocation"""
    print("=== GPU Memory and Model Status ===")
    
    status = gpu_manager.get_status()
    
    # GPU Memory Information
    print("\nGPU Memory Usage:")
    for gpu_id, info in status["gpu_memory"].items():
        print(f"  GPU {gpu_id}:")
        print(f"    Total: {info['total']:.1f} GB")
        print(f"    Allocated: {info['allocated']:.1f} GB")
        print(f"    Reserved: {info['reserved']:.1f} GB")
        print(f"    Free: {info['free']:.1f} GB")
        print(f"    Utilization: {info['utilization']:.1f}%")
    
    # Model Status
    print(f"\nModel Status ({status['loaded_models']}/{status['total_models']} loaded):")
    for model_name, model_info in status["models"].items():
        status_icon = "✓" if model_info["loaded"] else "✗"
        print(f"  {status_icon} {model_name} (GPU {model_info['gpu']}, {model_info['type']})")
    
    return status

if __name__ == "__main__":
    monitor_gpu_usage()
EOF

chmod +x /opt/citadel/scripts/gpu_monitor.py

# Create model loading test script
cat > /opt/citadel/scripts/test_model_loading.py << 'EOF'
#!/usr/bin/env python3
import sys
sys.path.append('/opt/citadel/services')
from gpu_manager import gpu_manager
import time

def test_model_loading():
    """Test model loading and GPU allocation"""
    print("=== Testing Model Loading ===")
    
    # Initial status
    print("\nInitial GPU status:")
    gpu_manager.get_status()
    
    # Load all models
    print("\nLoading all models...")
    results = gpu_manager.load_all_models()
    
    for model_name, success in results.items():
        status = "✓" if success else "✗"
        print(f"  {status} {model_name}")
    
    # Check final status
    print("\nFinal GPU status:")
    status = gpu_manager.get_status()
    
    # Test model inference
    print("\nTesting model inference...")
    
    # Test sentence transformer models
    for model_name in ["all-MiniLM-L6-v2", "e5-small", "bge-base"]:
        model = gpu_manager.get_model(model_name)
        if model:
            try:
                embedding = model.encode("test sentence")
                print(f"  ✓ {model_name}: Generated embedding (dim: {len(embedding)})")
            except Exception as e:
                print(f"  ✗ {model_name}: Inference failed - {e}")
    
    # Test transformers model
    model = gpu_manager.get_model("phi-3-mini")
    tokenizer = gpu_manager.get_tokenizer("phi-3-mini")
    if model and tokenizer:
        try:
            inputs = tokenizer("test sentence", return_tensors="pt")
            # Move inputs to same device as model
            device = next(model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            import torch
            with torch.no_grad():
                outputs = model(**inputs)
            print(f"  ✓ phi-3-mini: Generated output (shape: {outputs.last_hidden_state.shape})")
        except Exception as e:
            print(f"  ✗ phi-3-mini: Inference failed - {e}")
    
    return True

if __name__ == "__main__":
    test_model_loading()
EOF

chmod +x /opt/citadel/scripts/test_model_loading.py
```

### Validation Commands
```bash
# Test GPU memory allocation
python /opt/citadel/scripts/gpu_monitor.py

# Test model loading
python /opt/citadel/scripts/test_model_loading.py

# Monitor GPU utilization
watch -n 2 'nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv'
```

---

*[Document continues with remaining tasks 2.3-5.4 following the same detailed format, including FastAPI Embedding Service Development, Model Performance Optimization, Model Management API, and all subsequent phases through Phase 5: Monitoring and R&D Handoff]*

---

**Document Status:** Enhanced implementation guide with expanded scope  
**Total Tasks:** 39 tasks across 6 phases  
**Estimated Duration:** ~65 hours (4-5 weeks)  
**Key Enhancements:** GraphQL/gRPC APIs, automated testing, scalability testing, PRD traceability  
**Testing Framework:** pytest + Locust for comprehensive validation

